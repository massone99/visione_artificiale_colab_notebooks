{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/massone99/visione_artificiale_colab_notebooks/blob/main/GroceryProductDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Grocery Product Detection**\n",
        "Obiettivo dell’esercitazione è la realizzazione di un sistema di **localizzazione di prodotti negli scaffali dei supermercati**. Per ciascun prodotto è disponibile un singolo template.\n",
        "\n",
        "L'approccio adottato è basato su **template matching rigido con feature**; viene fatta scorrere sull'immagine in input una finestra di dimensioni coincidenti con quelle del template e per ciascuna sottofinestra viene valutata la compatibilità con il template (colore e descrittori locali).\n",
        "\n",
        "La ricerca deve prevedere un’analisi **multiscala** perché la dimensione del template non coincide esattamente con la dimensione del prodotto nelle immagini degli scaffali.\n",
        "\n",
        "<img src=https://biolab.csr.unibo.it/vr/esercitazioni/NotebookImages/EsProductDetection/Detection.png width=\"800\">\n"
      ],
      "metadata": {
        "id": "7LO3Kaovxdr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import delle librerie**\n",
        "È necessario ora eseguire l'import delle librerie utilizzate durante l'esercitazione.\n",
        "Per questa esercitazione è necessario inoltre utilizzare una versione specifica della libreria opencv perché si utilizzano feature (SIFT) che nelle versioni successive della libreria sono disponibili solo a pagamento."
      ],
      "metadata": {
        "id": "oYnkgsubW0N6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  opencv-python\n",
        "!pip install  opencv-contrib-python\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "akSD2vbs0A1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset**\n",
        "\n",
        "Il dataset per l'esercitazione è un piccolo sottoinsieme di immagini tratte dal dataset Grozy; vengono forniti qui i template di tre prodotti e, per ciascun prodotto, qualche foto di scaffali del supermercato che contengono quel prodotto."
      ],
      "metadata": {
        "id": "q1bnNUjcW2qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://bias.csr.unibo.it/VR/Esercitazioni/DBs/ProductDetection.zip\n",
        "!unzip /content/ProductDetection.zip\n",
        "!rm /content/ProductDetection.zip\n"
      ],
      "metadata": {
        "id": "GEnnkwOcYyaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carichiamo il template di un prodotto..."
      ],
      "metadata": {
        "id": "GTI__3mkY22_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeTcR5EFxJHf"
      },
      "outputs": [],
      "source": [
        "product_template = cv2.imread('/content/ProductDetection/P1.png')\n",
        "cv2_imshow(product_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "...e un'immagine all'interno della quale effettuare la ricerca."
      ],
      "metadata": {
        "id": "shC3zodFf-8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = cv2.imread('/content/ProductDetection/P1_1122.png')\n",
        "cv2_imshow(input_image)"
      ],
      "metadata": {
        "id": "32R1S7wRgAiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Selezione dei keypoint**\n",
        "\n",
        "Per la rappresentazione si utilizzeranno descrittori locali SIFT calcolati in corrispondenza di keypoint individuati attraverso un processo di **dense sampling a nido d’ape**.\n",
        "\n",
        "<img src=https://biolab.csr.unibo.it/vr/esercitazioni/NotebookImages/EsProductDetection/Sampling.png width=\"800\">\n",
        "\n",
        "La **spaziatura** tra i keypoint viene fissata a 16 pixel.\n",
        "\n",
        "A ciascun keypoint viene inoltre assegnato un valore di **scala di default** pari a 3.0 che verrà poi utilizzato per il calcolo del descrittore associato al keypoint."
      ],
      "metadata": {
        "id": "wse4_Kc3XC1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def kp_dense_sampling(img, spacing, scale):\n",
        "    # Initialize an empty list to store keypoints\n",
        "    points = []\n",
        "\n",
        "    # Get the shape of the input image\n",
        "    s = img.shape\n",
        "\n",
        "    # Calculate the number of keypoints per column and row based on spacing\n",
        "    points_per_col = int(s[0] / spacing)\n",
        "    points_per_row = int(s[1] / spacing)\n",
        "\n",
        "    # Calculate half of the spacing\n",
        "    half_spacing = spacing / 2\n",
        "\n",
        "    # Calculate the borders on the X and Y axes\n",
        "    borderX = (s[1] - (points_per_row - 1) * spacing) / 2\n",
        "    borderY = (s[0] - (points_per_col - 1) * spacing) / 2\n",
        "\n",
        "    # Initialize the Y position for the keypoints\n",
        "    posY = borderY\n",
        "\n",
        "    # Loop through the rows (Y)\n",
        "    for y in range(points_per_col):\n",
        "        # Check if the current row is even or odd to adjust the X position\n",
        "        if y % 2 == 0:\n",
        "            posX = borderX\n",
        "        else:\n",
        "            posX = half_spacing + borderX\n",
        "\n",
        "        # Loop through the columns (X) within the current row\n",
        "        for x in range(points_per_row):\n",
        "            # Check if the current position is within the image boundaries\n",
        "            if posX <= s[1] - borderX and posY <= s[0] - borderY and posX >= 0 and posY >= 0:\n",
        "                # Create a KeyPoint object and add it to the list of keypoints\n",
        "                p = cv2.KeyPoint(posX, posY, scale)\n",
        "                points.append(p)\n",
        "\n",
        "            # Increment the X position by the spacing\n",
        "            posX += spacing\n",
        "\n",
        "        # Increment the Y position by the spacing\n",
        "        posY += spacing\n",
        "\n",
        "    # Return the list of keypoints\n",
        "    return points\n",
        "\n",
        "# Call the kp_dense_sampling function with specific parameters\n",
        "keypoints = kp_dense_sampling(product_template, 16, 3.0)\n",
        "\n",
        "# Create a copy of the original image for visualization\n",
        "img_sampling = product_template.copy()\n",
        "\n",
        "# Draw the keypoints on the copy of the image\n",
        "cv2.drawKeypoints(product_template, keypoints, img_sampling, (0, 0, 255))\n",
        "\n",
        "# Display the image with keypoints using OpenCV's imshow function\n",
        "cv2_imshow(img_sampling)"
      ],
      "metadata": {
        "id": "Pn9GkhfJDKdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre-selezione dei candidati in base al colore**\n",
        "Al fine di velocizzare la procedura di ricerca del prodotto viene fatta una pre-selezione dei candidati sulla base dell'analisi dell'istogramma colore.\n",
        "\n",
        "In particolare, solo le sottofinestre che presentano un istogramma colore \"compatibile\" con quello del template vengono ulteriormente valutate sulla base della similarità dei descrittori locali.\n",
        "\n",
        "L'istogramma colore viene calcolato sui canali Cb e Cr dello spazio YCbCr. La compatibilità viene valutata dalla funzione `check_color_similarity` che si occupa di verificare che l'intersezione tra l'istogramma del template e quello della sottofinestra in oggetto sia superiore alla soglia prefissata `colorhist_intersection_thr`."
      ],
      "metadata": {
        "id": "_WpJrgZ9XHnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of bins for the color histogram\n",
        "bin_count = 20\n",
        "\n",
        "# Define a threshold for color histogram intersection for similarity check\n",
        "colorhist_intersection_thr = 1.1\n",
        "\n",
        "def compute_hist(img):\n",
        "    image_ycbcr = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
        "    # Estrai i canali Cb e Cr\n",
        "    # channel_y = image_ycbcr[:, :, 0]  # Canale Y (canale 0)\n",
        "    channel_cb = image_ycbcr[:, :, 1]  # Canale Cb (canale 1)\n",
        "    channel_cr = image_ycbcr[:, :, 2]  # Canale Cr (canale 2)\n",
        "\n",
        "    # Calcola l'istogramma per il canale Cb\n",
        "    hist_cb = cv2.calcHist([channel_cb], [0], None, [bin_count], [0, 256])\n",
        "\n",
        "    # Calcola l'istogramma per il canale Cr\n",
        "    hist_cr = cv2.calcHist([channel_cr], [0], None, [bin_count], [0, 256])\n",
        "\n",
        "    # Unisci i due istogrammi in un unico istogramma\n",
        "    hist = np.concatenate((hist_cb, hist_cr), axis=0)\n",
        "\n",
        "    return hist\n",
        "\n",
        "# Function to check the color similarity between a product histogram and an image\n",
        "def check_color_similarity(product_hist, img):\n",
        "    # Compute the color histogram for the input image\n",
        "    img_hist = compute_hist(img)\n",
        "\n",
        "    # Calculate the intersection of histograms using element-wise minimum and sum\n",
        "    intersection = np.minimum(product_hist, img_hist).sum()\n",
        "\n",
        "    # Check if the intersection value is greater than the specified threshold\n",
        "    if intersection > colorhist_intersection_thr:\n",
        "        # If the intersection is above the threshold, consider the colors similar\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "4gxvT1fDC70U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ricerca dei candidati con analisi multi-scala**\n",
        "La ricerca del prodotto nell'immagine va fatta con un'analisi multiscala.\n",
        "\n",
        "Per motivi di efficienza si preferisce non riscalare il template ma **riscalare l'immagine in input**. Questo permette di calcolare l'istogramma colore, i keypoint e i descrittori locali del template una sola volta.\n",
        "\n",
        "Il template dunque è fisso, si riscala l'immagine in input e su ogni immagine riscalata viene fatta scorrere una **finestra di dimensione pari a quella del template**. Ad ogni step la finestra si sposta di un numero di pixel (in orizzontale e in verticale) pari a una percentuale della dimensione del template (parametro `window_step_perc`); in particolare, la percentuale va calcolata rispetto alla larghezza e all'altezza del template rispettivamente per lo spostamento in orizzontale e in verticale.\n",
        "\n",
        "<img src=https://biolab.csr.unibo.it/vr/esercitazioni/NotebookImages/EsProductDetection/Scala.png width=\"800\">\n",
        "\n",
        "Si richiede di implementare la funzione `find_product_candidates` che riceve in input:\n",
        "\n",
        "* `product_template` - l'immagine del template (BGR)\n",
        "* `img` - l'immagine di input nella quale effettuare la ricerca\n",
        "* `keypoints` - l'array di keypoint da utilizzare per il calcolo dei descrittori\n",
        "* `scales` - l'array di fattori di scala da applicare\n",
        "\n",
        "Gli step da implementare sono i seguenti.\n",
        "\n",
        "**Rappresentazione del template**\n",
        "\n",
        "A partire dall'immagine del template si deve calcolare il suo istogramma colore (funzione `compute_hist` implementata precedentemente) e l'insieme dei suoi descrittori locali.\n",
        "\n",
        "Per il calcolo dei descrittori è necessario istanziare un estrattore SIFT come segue:\n",
        "\n",
        "`sift = cv2.xfeatures2d.SIFT_create()`\n",
        "\n",
        "I descrittori associati a un insieme di keypoint prefissato (`keypoints`) possono essere calcolati richiamando il metodo `sift.compute()` passandogli come parametri l'**immagine grayscale** e il set di keypoint.\n",
        "\n",
        "**Scansione dell'immagine e confronto col template**\n",
        "\n",
        "Per ogni sottofinestra individuata durante la scansione si dovrà:\n",
        "* verificare la **similarità dell'istogramma** colore con l'istogramma del template;\n",
        "* se la similarità a livello di colore è sufficiente, calcolare i **descrittori locali** e confrontarli con quelli del template, calcolando la **distanza Euclidea tra punti corrispondenti** e mediando infine la somma ottenuta sul numero di keypoint;\n",
        "* se la **distanza media** tra descrittori è inferiore alla soglia prefissata (`dist_thr`), istanziare un candidato, memorizzando le coordinate della sottofinestra (top, left, bottom, right) e il valore di distanza media calcolato:\n",
        "\n",
        "  `r = {\"top\": ?, \"bottom\": ?, \"left\": ?, \"right\": ?, \"dist\": ?}`\n",
        "  \n",
        "  **Attenzione!** Le coordinate della finestra vanno espresse rispetto all'immagine alla scala originale, tenete quindi in considerazione il fattore di scala..."
      ],
      "metadata": {
        "id": "9S85ZHZcXkCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_avg_descriptor_distance(descriptor_set_1, descriptor_set_2):\n",
        "    total_distance = 0.0\n",
        "\n",
        "    # Iterate over SIFT descriptors and compute the distance\n",
        "    for i in range(descriptor_set_1[1].shape[0]):\n",
        "        descriptor_distance = np.linalg.norm(descriptor_set_1[1][i] - descriptor_set_2[1][i])\n",
        "        total_distance += descriptor_distance\n",
        "\n",
        "    # Calculate the average distance\n",
        "    average_distance = total_distance / (descriptor_set_1[1].shape[0] * descriptor_set_1[1].shape[1])\n",
        "    return average_distance\n",
        "\n",
        "def find_product_candidates(product_template, input_image, keypoints, search_scales):\n",
        "    # Define the step size for the sliding window and the distance threshold\n",
        "    window_step_percentage = 0.05\n",
        "    distance_threshold = 4.5\n",
        "    candidate_list = []  # Store the candidates that meet the criteria\n",
        "\n",
        "    # Compute the histogram of the product template (assuming you have this function)\n",
        "    template_histogram = compute_hist(product_template)\n",
        "    sift = cv2.SIFT_create()\n",
        "\n",
        "    # Convert the product template to grayscale and compute SIFT descriptors\n",
        "    template_gray = cv2.cvtColor(product_template, code=cv2.COLOR_BGR2GRAY)\n",
        "    template_descriptors = sift.compute(template_gray, keypoints)\n",
        "\n",
        "    # Check for color similarity between the product template and the input image\n",
        "    if check_color_similarity(template_histogram, input_image):\n",
        "        # Iterate over different scales of the input image\n",
        "        for scale in search_scales:\n",
        "            # Resize the input image based on the current scale\n",
        "            scaled_image = cv2.resize(input_image, None, fx=scale, fy=scale)\n",
        "            scaled_image_gray = cv2.cvtColor(scaled_image, code=cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Get the dimensions of the scaled image\n",
        "            scaled_height, scaled_width = scaled_image_gray.shape\n",
        "\n",
        "            # Iterate over the image with a sliding window\n",
        "            for top in range(0, scaled_height, int(window_step_percentage * scaled_height)):\n",
        "                for left in range(0, scaled_width, int(window_step_percentage * scaled_width)):\n",
        "                    bottom = top + product_template.shape[0]\n",
        "                    right = left + product_template.shape[1]\n",
        "\n",
        "                    # Ensure the sliding window stays within the image boundaries\n",
        "                    if bottom <= scaled_height and right <= scaled_width:\n",
        "                        # Extract the region defined by the sliding window\n",
        "                        window = scaled_image_gray[top:bottom, left:right]\n",
        "\n",
        "                        # Compute SIFT descriptors for the current window\n",
        "                        window_descriptors = sift.compute(window, keypoints)\n",
        "\n",
        "                        # Calculate the average distance between SIFT descriptors using calculate_avg_descriptor_distance\n",
        "                        avg_distance = calculate_avg_descriptor_distance(template_descriptors, window_descriptors)\n",
        "\n",
        "                        # Check if the distance is below the threshold\n",
        "                        if avg_distance < distance_threshold:\n",
        "                            # Store the candidate's coordinates, scale, and distance\n",
        "                            candidate_info = {\n",
        "                                \"top\": top,\n",
        "                                \"bottom\": bottom,\n",
        "                                \"left\": left,\n",
        "                                \"right\": right,\n",
        "                                \"scale\": scale,\n",
        "                                \"distance\": avg_distance\n",
        "                            }\n",
        "                            candidate_list.append(candidate_info)\n",
        "\n",
        "    return candidate_list\n",
        "\n",
        "scales = np.arange(0.5, 1.5, 0.25)\n",
        "# Ricerca dei candidati\n",
        "candidates = find_product_candidates(product_template, input_image, keypoints, scales)\n",
        "# Visualizzazione dei candidati individuati\n",
        "initial_candidates = input_image.copy()\n",
        "for c in candidates:\n",
        "  cv2.rectangle(initial_candidates, (round(c[\"left\"]), round(c[\"top\"])), (round(c[\"right\"]), round(c[\"bottom\"])), (0,0,255))\n",
        "cv2_imshow(initial_candidates)"
      ],
      "metadata": {
        "id": "GoE9FAGmWyjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Semplificazione dell'output**\n",
        "\n",
        "La procedura di ricerca precedente può rilevare più candidati corrispondenti allo stesso prodotto; per ridurre il numero di falsi positivi prodotti dall'algoritmo è necessario semplificare l'output mantenendo solo i candidati più significativi.\n",
        "\n",
        "Si consiglia di implementare a tal fine una procedura di **soppressione dei non minimi** (avendo qui una misura di distanza). In particolare ogni candidato può essere confrontato con tutti gli altri e, se tra quelli che si **sovrappongono ad esso per più del 50%** (verificare boundind box) se ne trova uno con distanza inferiore, il candidato in esame va rimosso dal risultato."
      ],
      "metadata": {
        "id": "nnU1yiVPXc-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def non_minima_suppression(candidates):\n",
        "  result = []\n",
        "  # TODO\n",
        "  return result\n",
        "\n",
        "# Soppressione dei non minimi\n",
        "result = non_minima_suppression(candidates)\n",
        "# Visualizzazione del risultato finale\n",
        "final_candidates = initial_candidates.copy()\n",
        "for c in result:\n",
        "  cv2.rectangle(final_candidates, (round(c[\"left\"]), round(c[\"top\"])), (round(c[\"right\"]), round(c[\"bottom\"])), (0,255,0), thickness = 3)\n",
        "cv2_imshow(final_candidates)"
      ],
      "metadata": {
        "id": "UIYhQN0v6F9f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}