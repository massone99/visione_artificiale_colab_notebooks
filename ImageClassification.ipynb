{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/massone99/visione_artificiale_colab_notebooks/blob/main/ImageClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLTkeMfY8fkU"
      },
      "source": [
        "# **Esercitazione su image classification**\n",
        "Nell'esercitazione odierna utilizzeremo una *Convolutional Neural Network* (CNN) per applicazioni di riconoscimento di volti (*Face Recognition*).\n",
        "\n",
        "Faremo uso del framework **TensorFlow**, sfruttando la libreria open-source **Keras** appositamente progettata per permettere una rapida prototipazione di reti neurali profonde.\n",
        "\n",
        "Alcuni link di approfondimento:\n",
        "- Introduzione a TensorFlow con utile schema grafico delle [API disponibili](https://ekababisong.org/gcp-ml-seminar/tensorflow/#navigating-through-the-tensorflow-api)\n",
        "- [Keras](https://keras.io/)\n",
        "\n",
        "Nello specifico sarà utilizzata la rete [VGG-Face](https://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf) pre-addestrata sul [dataset VGG-Face](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/) (contenente oltre 2 milioni di immagini di volti appartenenti a più di 2000 soggetti).\n",
        "\n",
        "L'obiettivo dell'esercitazione è quello di utilizzare una CNN pre-addestrata come *feature extractor* per il riconoscimento di volti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlVcIij-B6z2"
      },
      "source": [
        "# **Operazioni preliminari**\n",
        "Prima di incominciare, è necessario eseguire alcune operazioni preliminari.\n",
        "\n",
        "Eseguendo la cella sottostante tutto il materiale necessario per lo svolgimento dell'esercitazione verrà scaricato sulla macchina remota. Alla fine dell'esecuzione selezionare il tab **Files** per verificare che tutto sia stato scaricato correttamente.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay4bpiweCCDb"
      },
      "source": [
        "!wget http://bias.csr.unibo.it/VR/Esercitazioni/DBs/ImageClassification/FaceScrubSubset_Celebrities.zip\n",
        "!wget http://bias.csr.unibo.it/VR/Esercitazioni/MaterialeEsImageClassification.zip\n",
        "!wget http://bias.csr.unibo.it/VR/Esercitazioni/PythonUtilities.zip\n",
        "\n",
        "!unzip -q /content/FaceScrubSubset_Celebrities.zip\n",
        "!unzip -q /content/MaterialeEsImageClassification.zip\n",
        "!unzip -q /content/PythonUtilities.zip\n",
        "\n",
        "!rm /content/FaceScrubSubset_Celebrities.zip\n",
        "!rm /content/MaterialeEsImageClassification.zip\n",
        "!rm /content/PythonUtilities.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAGlRsjXCCib"
      },
      "source": [
        "# **Import delle librerie**\n",
        "Per prima cosa è necessario eseguire l'import delle librerie utilizzate durante l'esecitazione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy9j5IO78fkV"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "import cv2\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import vr_utilities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4ogdD968fkZ"
      },
      "source": [
        "# **Dataset**\n",
        "Il dataset ultilizzato è composto da immagini RGB di volti di persone famose. In particolare utilizzeremo un sottoinsieme del [FaceScrub](http://vintage.winklerbros.net/facescrub.html) contenente 1590 immagini di 530 soggetti diversi (3  immagini per ciascuno di essi, 2 per il training e 1 per il test).\n",
        "\n",
        "Visto il numero esiguo di immagini (1060 per il dataset di training), cercare di addestrare da zero una CNN complessa (partendo da pesi random) risulta impossibile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ky7QZXOU8fka"
      },
      "source": [
        "db_path = '/content/Celebrities'\n",
        "train_filelist = 'TrainingSet.txt'\n",
        "test_filelist = 'TestSet.txt'\n",
        "labelnames_list = 'LabelNames.txt'\n",
        "\n",
        "print('Caricamento in corso ...')\n",
        "or_train_x, train_y = vr_utilities.load_labeled_dataset(train_filelist, db_path)\n",
        "or_test_x, test_y = vr_utilities.load_labeled_dataset(test_filelist, db_path)\n",
        "\n",
        "label_names = vr_utilities.load_label_names(labelnames_list, db_path)\n",
        "\n",
        "print('Shape training set:', or_train_x.shape)\n",
        "print('Shape test set:', or_test_x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc8AeObg8fke"
      },
      "source": [
        "La cella seguente contiene il codice per mostrare alcune immagini del training set. Guardando alcuni esempi si può facilmente notare la grande variabilità in termini di:\n",
        "- posa;\n",
        "- illuminazione;\n",
        "- espressione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "bHVK45PQ8fkf"
      },
      "source": [
        "rows = 3\n",
        "columns = 6\n",
        "\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "_, axs = plt.subplots(rows, columns, squeeze=False,figsize=(20, 10))\n",
        "samples = random.sample(range(len(label_names)), columns)\n",
        "\n",
        "for j in range(columns):\n",
        "    idx=samples[j]\n",
        "    sel_train_images=[or_train_x[k] for k in np.where(train_y==idx)[0]]\n",
        "    sel_test_images=[or_test_x[k] for k in np.where(test_y==idx)[0]]\n",
        "    sel_images=sel_train_images+sel_test_images\n",
        "    axs[0, j].set_title(label_names[idx])\n",
        "    for i in range(rows):\n",
        "        axs[i, j].axis('off')\n",
        "        axs[i, j].imshow(sel_images[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-59fss8s8fkj"
      },
      "source": [
        "# **VGG-Face**\n",
        "*VGG-Face* è una CNN introdotta per applicazioni di riconoscimento del volto.\n",
        "\n",
        "È composta da:\n",
        "- 16 *layer* di **convoluzione**;\n",
        "- 5 *layer* di **max pooling**.\n",
        "\n",
        "<img src=https://biolab.csr.unibo.it/vr/esercitazioni/NotebookImages/EsImageClassification\\vgg-face-architecture.jpg width=\"1000\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Definizione del modello**\n",
        "La funzione seguente crea il modello della *VGG-Face*."
      ],
      "metadata": {
        "id": "fYmiOMyEJib-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vggface():\n",
        "  model=keras.Sequential(\n",
        "          [\n",
        "              layers.Input(shape=(224,224,3),name='input'),\n",
        "              layers.Conv2D(filters=64, kernel_size=3,padding='same', activation='relu',name='conv1_1-relu1_1'),\n",
        "              layers.Conv2D(filters=64, kernel_size=3,padding='same', activation='relu',name='conv1_2-relu1_2'),\n",
        "              layers.MaxPooling2D(pool_size=2, strides=2,name='pool1'),\n",
        "              layers.Conv2D(filters=128, kernel_size=3,padding='same', activation='relu',name='conv2_1-relu2_1'),\n",
        "              layers.Conv2D(filters=128, kernel_size=3,padding='same', activation='relu',name='conv2_2-relu2_2'),\n",
        "              layers.MaxPooling2D(pool_size=2, strides=2,name='pool2'),\n",
        "              layers.Conv2D(filters=256, kernel_size=3,padding='same', activation='relu',name='conv3_1-relu3_1'),\n",
        "              layers.Conv2D(filters=256, kernel_size=3,padding='same', activation='relu',name='conv3_2-relu3_2'),\n",
        "              layers.Conv2D(filters=256, kernel_size=3,padding='same', activation='relu',name='conv3_3-relu3_3'),\n",
        "              layers.MaxPooling2D(pool_size=2, strides=2,name='pool3'),\n",
        "              layers.Conv2D(filters=512, kernel_size=3,padding='same', activation='relu',name='conv4_1-relu4_1'),\n",
        "              layers.Conv2D(filters=512, kernel_size=3,padding='same', activation='relu',name='conv4_2-relu4_2'),\n",
        "              layers.Conv2D(filters=512, kernel_size=3,padding='same', activation='relu',name='conv4_3-relu4_3'),\n",
        "              layers.MaxPooling2D(pool_size=2, strides=2,name='pool4'),\n",
        "              layers.Conv2D(filters=512, kernel_size=3,padding='same', activation='relu',name='conv5_1-relu5_1'),\n",
        "              layers.Conv2D(filters=512, kernel_size=3,padding='same', activation='relu',name='conv5_2-relu5_2'),\n",
        "              layers.Conv2D(filters=512, kernel_size=3,padding='same', activation='relu',name='conv5_3-relu5_3'),\n",
        "              layers.MaxPooling2D(pool_size=2, strides=2,name='pool5'),\n",
        "              layers.Conv2D(filters=4096, kernel_size=7, activation='relu',name='fc6-relu6'),\n",
        "              layers.Dropout(0.5,name='do6'),\n",
        "              layers.Conv2D(filters=4096, kernel_size=1, activation='relu',name='fc7-relu7'),\n",
        "              layers.Dropout(0.5,name='do7'),\n",
        "              layers.Conv2D(filters=2622, kernel_size=1,activation='softmax',name='fc8-prob'),\n",
        "              layers.Flatten(name='flatten'),\n",
        "          ]\n",
        "        )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "5J7d0pUBQ8Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creazione del modello**\n",
        "Il codice seguente crea una *VGG-Face* richiamando la funzione **build_vggface** definita sopra."
      ],
      "metadata": {
        "id": "nVnrn4AnK6ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=build_vggface()"
      ],
      "metadata": {
        "id": "-k5oqdjHLOcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ArcnpQjP8WN"
      },
      "source": [
        "## **Visualizzazione del modello**\n",
        "Eseguendo la cella seguente è possibile stampare un riepilogo testuale della struttura della rete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZivahXaPyma"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps6qWnbeQBI9"
      },
      "source": [
        "Se si preferisce una visualizzazione grafica, eseguire la cella seguente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea1FXd8QQB6V"
      },
      "source": [
        "keras.utils.plot_model(model,show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Caricamento dei pesi**\n",
        "I pesi della rete *VGG-Face* addestrata sul dataset VGG-Face sono stati resi disponibili e possono essere caricati senza dover ripetere l'addestramento.\n",
        "\n",
        "Tramite il metodo **load_weights** è possibile caricare all'interno del modello i pesi memorizzati all'interno di un file."
      ],
      "metadata": {
        "id": "rMLmws9IPhdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('vgg_face_weights.h5')"
      ],
      "metadata": {
        "id": "tLssHCgcPlJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRdbWMTU2YIk"
      },
      "source": [
        "# **Creazione dell'estrattore di feature**\n",
        "Il dataset che utilizzeremo nell'esercitazione contiene soggetti diversi rispetto a quelli utilizzati per addestrare la VGG-Face.\n",
        "\n",
        "L'addestramento di una CNN su un nuovo problema, richiede un hardware sufficientemente potente e un training set etichettato di notevoli dimensioni.\n",
        "\n",
        "In alternativa al training da zero, possiamo utilizzare una rete esistente (pre-trained) per estrarre le feature generate ai livelli intermedi durante il passo forward ([*Transfer Learning*](https://cs231n.github.io/transfer-learning/)).\n",
        "\n",
        "<img src=https://biolab.csr.unibo.it/vr/esercitazioni/NotebookImages/EsImageClassification\\TransferLearning.png width=\"600\">\n",
        "\n",
        "Le feature possono essere utilizzate per:\n",
        "1. addestrare un classificatore esterno (es. SVM) a\n",
        "riconoscere i pattern del nuovo dominio applicativo;\n",
        "2. stimare il grado di similarità tra feature estratte da immagini differenti utilizzando una metrica (es. distanza euclidea o distanza coseno).\n",
        "\n",
        "L'operazione di estrazione delle feature consiste nel calcolare, per ogni immagine fornita in input, l'output della rete al livello desiderato (*layer_name*).\n",
        "\n",
        "Per evitare, durante il passo *forward*, di attraversare livelli non necessari della rete si può creare una nuova istanza della classe [**Model**](https://keras.io/api/models/model/) il cui input sara il medesimo del modello originale mentre l'ouput sarà rappresentato dal livello da cui si vogliono estrarre le feature (*layer_name*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy_U3R3hiB3N"
      },
      "source": [
        "layer_name='fc6-relu6'\n",
        "#layer_name='fc7-relu7'\n",
        "\n",
        "feature_extractor = keras.Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n",
        "\n",
        "print('Inputs: %s' % feature_extractor.inputs)\n",
        "print('Outputs: %s' % feature_extractor.outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkO3oqkG6vB8"
      },
      "source": [
        "## **Visualizzazione del nuovo modello**\n",
        "Eseguendo la cella seguente è possibile stampare un riepilogo testuale della struttura della rete da utilizzare come feature extractor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lpNZ35z7R4m"
      },
      "source": [
        "feature_extractor.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sFFg8zC7Vvr"
      },
      "source": [
        "Se si preferisce una visualizzazione grafica, eseguire la cella seguente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KloTr8Gp7X6_"
      },
      "source": [
        "keras.utils.plot_model(feature_extractor,show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PMaxhcw8fkx"
      },
      "source": [
        "# ***Pre-processing* delle immagini**\n",
        "Il modello di VGG-Face di cui abbiamo caricato i pesi è stato addestrato con delle immagini pre-elaborate. Sarà necessario eseguire le medesime operazioni sia sul training che sul test set prima di poterli utilizzare.\n",
        "\n",
        "Le immagini del nostro dataset presentano un'intensità luminosa nel range [0;1]. Per poter essere utilizzate con la rete pre-addestrata dovremo preventivamente \"mappare\" l'intensità nel range [-1;1]. Si esegua la cella seguente per effettuare tale *mapping*."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Range originale: [',np.min(or_train_x),';',np.max(or_train_x),']')\n",
        "\n",
        "norm_train_x=(or_train_x*2)-1\n",
        "norm_test_x=(or_test_x*2)-1\n",
        "\n",
        "print('Range ri-mappato: [',np.min(norm_train_x),';',np.max(norm_train_x),']')"
      ],
      "metadata": {
        "id": "8lzIs4gnIAS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvMRpZgk8fk1"
      },
      "source": [
        "# **Estrazione delle feature**\n",
        "Per estrarre le feature è sufficiente richiamare il metodo [**predict(...)**](https://keras.io/api/models/model_training_apis/#predict-method) del nostro estrattore (*feature_extractor*).\n",
        "\n",
        "Eseguire la cella seguente per estrarre le feature dal training e dal test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeuTWTQHZUOy"
      },
      "source": [
        "print('Estrazione delle feature...')\n",
        "train_features_x=feature_extractor.predict(norm_train_x)\n",
        "test_features_x=feature_extractor.predict(norm_test_x)\n",
        "\n",
        "print('Shape ndarray delle feature di train: ', train_features_x.shape)\n",
        "print('Shape ndarray delle feature di test: ', test_features_x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD24wSIJUjdC"
      },
      "source": [
        "Per comodità, può essere utile rimuovere le dimensioni unitarie tramite la funzione [**squeeze(...)**](https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html) di NumPy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAbGZkPfUmIy"
      },
      "source": [
        "train_features_x=np.squeeze(train_features_x)\n",
        "test_features_x=np.squeeze(test_features_x)\n",
        "\n",
        "print('Shape ndarray delle feature di train: ', train_features_x.shape)\n",
        "print('Shape ndarray delle feature di test: ', test_features_x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZstXN7t8fk5"
      },
      "source": [
        "# **Face Recognition**\n",
        "Le feature appena estratte possono essere direttamente utilizzate insieme alla [distanza coseno](https://en.wikipedia.org/wiki/Cosine_similarity) per effettuare *face recognition* sulle nostre immagini.\n",
        "\n",
        "Dati due vettori **a** e **b**, la distanza coseno può essere calcolata come:\n",
        "\n",
        "\\begin{align}\n",
        "D_C(\\mathbf{a},\\mathbf{b})=1-\\frac{\\mathbf{a} \\cdot{} \\mathbf{b}}{\\lVert \\mathbf{a} \\rVert \\lVert \\mathbf{b} \\rVert}\n",
        "\\end{align}\n",
        "\n",
        "La funzione **compute_cosine_distances(...)**, definita nella cella seguente, calcola le distanze coseno delle feature di una immagine di test (*query_features_x*) da tutte le feature del training set (*train_features_x*). Questa implementazione permette di calcolare la norma di ogni immagine di test una sola volta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u-0EKpbqRck"
      },
      "source": [
        "def compute_cosine_distances(train_features_x,query_features_x):\n",
        "  cosine_distances=[]\n",
        "  norm_query=np.linalg.norm(query_features_x)\n",
        "  for train_feature in train_features_x:\n",
        "    norm_train=np.linalg.norm(train_feature)\n",
        "    cos_dist=1-np.dot(query_features_x, train_feature)/(norm_query*norm_train)\n",
        "    cosine_distances.append(cos_dist)\n",
        "\n",
        "  return np.asarray(cosine_distances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sas_5lYi8fk9"
      },
      "source": [
        "## **Test**\n",
        "La cella sottostante calcola tutte le distanze coseno tra le feature del dataset di test e quelle di training memorizzandole nella variabile *test_distances*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HE-EkeiryiB"
      },
      "source": [
        "test_distances=[]\n",
        "print('Calcolo distanze coseno ...')\n",
        "for test_features in test_features_x:\n",
        "  test_distances.append(compute_cosine_distances(train_features_x,test_features))\n",
        "\n",
        "test_distances=np.asarray(test_distances)\n",
        "\n",
        "print('Shape ndarray delle distanze: ', test_distances.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L41bf0mJXr1w"
      },
      "source": [
        "È possibile misurare l'accuratezza del sistema di *face recognition* implementato eseguendo la cella successiva."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY1OgW8bU2JP"
      },
      "source": [
        "test_distances_sorted_indices=np.argsort(test_distances,axis=1)\n",
        "\n",
        "predicted_y=train_y[test_distances_sorted_indices[:,0]]\n",
        "\n",
        "errors = predicted_y != test_y\n",
        "\n",
        "accuracy=1-(errors.sum()/len(errors))\n",
        "print('Accuracy sul test set: %.3f' % (accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5VZ8aLt8flC"
      },
      "source": [
        "## **Visualizzazione errori**\n",
        "La cella seguente permette di visualizzare le immagini di test che vengono classificate in maniera errata. Sopra ad ogni immagine è riportato il nome del soggetto mentre a lato le classi più probabili."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSftDCdgW6lC"
      },
      "source": [
        "error_indices = np.where(errors == True)[0]\n",
        "\n",
        "if error_indices.shape[0] > 0:\n",
        "  # Visualizzazione immagini\n",
        "  image_per_row = 2\n",
        "  top_class_count = 5\n",
        "\n",
        "  row_count=math.ceil(len(error_indices)/image_per_row)\n",
        "  column_count=image_per_row\n",
        "  plt.rcParams.update({'font.size': 12})\n",
        "  _, axs = plt.subplots(row_count, column_count,figsize=(20, 4*row_count),squeeze=False)\n",
        "\n",
        "  for i in range(row_count):\n",
        "    for j in range(column_count):\n",
        "      axs[i,j].axis('off')\n",
        "\n",
        "  for i in range(len(error_indices)):\n",
        "    q = i // image_per_row\n",
        "    r = i % image_per_row\n",
        "    idx = error_indices[i]\n",
        "\n",
        "    axs[q,r].imshow(or_test_x[idx])\n",
        "    axs[q,r].set_title(label_names[test_y[idx]])\n",
        "\n",
        "    best_indices=test_distances_sorted_indices[idx,0:2*top_class_count]\n",
        "    best_distances=test_distances[idx,best_indices]\n",
        "\n",
        "    best_y = train_y[best_indices]\n",
        "    _, unique_indices = np.unique(best_y, return_index=True)\n",
        "    unique_indices=np.sort(unique_indices)\n",
        "\n",
        "    text=''\n",
        "    for j in range(top_class_count):\n",
        "        text+='{}: {:.3f}\\n'.format(label_names[best_y[unique_indices[j]]],best_distances[unique_indices[j]])\n",
        "\n",
        "    axs[q,r].text(330, 150, text, horizontalalignment='left', verticalalignment='center')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWbGlEIM8flG"
      },
      "source": [
        "# **Esercizio 1**\n",
        "Utilizzare il sistema implementato per verificare a quale tra le celebrità presenti nel dataset assomigliate maggiormente.\n",
        "\n",
        "A tal fine:\n",
        "\n",
        "1. scattare una foto con il proprio volto in primo piano;\n",
        "2. ritagliarla per ottenere un'immagine quadrata (rapporto 1:1);\n",
        "3. riscalare l'immagine a una dimensione 224 x 224 pixel;\n",
        "4. trasferire l'immagine ottenuta su **Colab** utilizzando la funzione *Upload* del tab **Files**;\n",
        "5. caricare l'immagine in una variabile (per farlo può essere utile la funzione [**imread(...)**](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imread.html) della libreria [**Matplotlib**](https://matplotlib.org/));\n",
        "6. effettuare il *pre-processing* dell'immagine;\n",
        "7. estrarre le feature utilizzando l'estrattore creato;\n",
        "8. calcolare la distanza coseno tra le feature estratte e quelle del training set;\n",
        "9. visualizzare, utilizzando la libreria Matplotlib, il nome e le foto delle 3 celebrità più somiglianti."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaeohxQbBmbP"
      },
      "source": [
        "image_file_path = #...\n",
        "similar_count = 3\n",
        "\n",
        "# Punto 5\n",
        "my_image = #...\n",
        "\n",
        "# Punto 6\n",
        "my_norm_image = #...\n",
        "\n",
        "# Punto 7\n",
        "my_image_features= #...\n",
        "my_image_features=np.squeeze(my_image_features)\n",
        "\n",
        "# Punto 8\n",
        "my_distances= #...\n",
        "\n",
        "# Punto 9\n",
        "my_distances_sorted_indices=np.argsort(my_distances)\n",
        "best_indices=my_distances_sorted_indices[0:similar_count]\n",
        "best_distances=my_distances[best_indices]\n",
        "\n",
        "plt.imshow((my_image))\n",
        "plt.axis('off')\n",
        "\n",
        "best_y = train_y[best_indices]\n",
        "\n",
        "text=''\n",
        "for j in range(similar_count):\n",
        "    text+='{}: {:.3f}\\n'.format(label_names[best_y[j]],best_distances[j])\n",
        "\n",
        "plt.text(my_image.shape[1]*1.05, my_image.shape[0]*0.5, text, horizontalalignment='left', verticalalignment='center')\n",
        "\n",
        "similar_face_indices=[i for i in best_indices if train_y[i] in best_y[:similar_count]]\n",
        "\n",
        "plt.rcParams.update({'font.size': 13})\n",
        "_, axs = plt.subplots(1, len(similar_face_indices),figsize=(20, 4))\n",
        "\n",
        "for i in range(len(similar_face_indices)):\n",
        "    idx=similar_face_indices[i]\n",
        "    axs[i].axis('off')\n",
        "    axs[i].imshow(or_train_x[idx])\n",
        "    axs[i].set_title(label_names[train_y[idx]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Esercizio 2**\n",
        "Ripetere l'esercizio 1 utilizzando le feature restituite dal layer *fc7-relu7* anziché quelle del layer *fc6-relu6*."
      ],
      "metadata": {
        "id": "tlDoJZMFNlv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Esercizio 3**\n",
        "Addestrare un classificatore (es. SVM) a riconoscere le classi del nostro dataset.\n",
        "\n",
        "Per svolgere l'esercizio si suggerisce di:\n",
        "1. per ridurre i tempi di calcolo, ridurre la dimensionalità dei vettori di feature dei dataset di training e test. Per farlo si consiglia di utilizzare la classe [**PCA**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) messa a disposizione dalla libreria **Scikit-learn** e in particolare i metodi [**fit**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit) e [**transform**](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.transform);\n",
        "2. creare un classificatore utilizzando le classi messe a disposizione da **OpenCV** (si veda esercitazione introduttiva);\n",
        "3. addestrare il classificatore passando le feature di training (a dimensionalità ridotta) al metodo **train**;\n",
        "4. valutare le prestazioni del classificatore appena addestrato utilizzando il metodo **predict** sul dataset di test (a dimensionalità ridotta);\n",
        "5. ridurre la dimensionalità delle feature estratte dalla foto utilizzata nell'Esercizio 1 utilizzando la **PCA** del punto 1;\n",
        "6. predire la classe di appartenenza più probabile delle feature estratte dalla foto utilizzata nell'Esercizio 1;\n",
        "7. visualizzare, utilizzando la libreria Matplotlib, il nome e le foto della celebrità a voi più somigliante.\n"
      ],
      "metadata": {
        "id": "nc5bk5WhOB8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Punto 1\n",
        "print('Shape delle feature di train: ', train_features_x.shape)\n",
        "print('Shape delle feature di test: ', test_features_x.shape)\n",
        "pca = #...\n",
        "#...\n",
        "red_train_features_x= #...\n",
        "red_test_features_x= #...\n",
        "print('Shape delle feature di train ridotte: ', red_train_features_x.shape)\n",
        "print('Shape delle feature di test ridotte: ', red_test_features_x.shape)\n",
        "\n",
        "# Punto 2\n",
        "classifier=#...\n",
        "#...\n",
        "\n",
        "# Punto 3\n",
        "#...\n",
        "\n",
        "# Punto 4\n",
        "_,pred_y= #...\n",
        "pred_y=np.squeeze(pred_y)\n",
        "errors = pred_y != test_y\n",
        "accuracy=1-(errors.sum()/len(errors))\n",
        "print('Accuracy sul test set: %.3f' % (accuracy))\n",
        "\n",
        "# Punto 5\n",
        "red_my_image_features= #...\n",
        "\n",
        "# Punto 6\n",
        "_,my_pred_y= #...\n",
        "\n",
        "# Punto 7\n",
        "my_pred_y=np.int32(my_pred_y)[0]\n",
        "pred_class_train_pos=np.where(train_y==my_pred_y)[0]\n",
        "pred_class_test_pos=np.where(test_y==my_pred_y)[0]\n",
        "\n",
        "_, axs = plt.subplots(1, 4,figsize=(20, 4))\n",
        "\n",
        "axs[0].axis('off')\n",
        "axs[0].imshow(my_image)\n",
        "\n",
        "axs[1].axis('off')\n",
        "axs[1].imshow(or_train_x[pred_class_train_pos[0]])\n",
        "axs[1].set_title(label_names[my_pred_y][0])\n",
        "\n",
        "axs[2].axis('off')\n",
        "axs[2].imshow(or_train_x[pred_class_train_pos[1]])\n",
        "axs[2].set_title(label_names[my_pred_y][0])\n",
        "\n",
        "axs[3].axis('off')\n",
        "axs[3].imshow(or_test_x[pred_class_test_pos[0]])\n",
        "axs[3].set_title(label_names[my_pred_y][0])"
      ],
      "metadata": {
        "id": "RK4n8DP3TK7C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}