{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/massone99/visione_artificiale_colab_notebooks/blob/main/ObjectTracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Esercitazione su object tracking**\n",
        "Nell'esercitazione odierna implementeremo due applicazioni di *object tracking* utilizzando rispettivamente gli algoritmi di *Mean-Shift* e di sottrazione del backgorund tramite *Mixture Of Gaussian*."
      ],
      "metadata": {
        "id": "z-InawVNR5fx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Operazioni preliminari**\n",
        "Eseguendo la cella sottostante tutto il materiale necessario per lo svolgimento dell'esercitazione verrà scaricato sulla macchina remota. Alla fine dell'esecuzione selezionare nel menù laterale **File** per verificare che tutto sia stato scaricato correttamente."
      ],
      "metadata": {
        "id": "8Bc6fWGWS69e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://bias.csr.unibo.it/VR/Esercitazioni/MaterialeEsObjectTracking.zip\n",
        "\n",
        "!unzip /content/MaterialeEsObjectTracking.zip\n",
        "\n",
        "!rm /content/MaterialeEsObjectTracking.zip"
      ],
      "metadata": {
        "id": "31or199_dH8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import delle librerie**\n",
        "Per prima cosa è necessario eseguire l'import delle librerie utilizzate durante l'esecitazione."
      ],
      "metadata": {
        "id": "HUvPhpHtTI8n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F8UddEOcmJt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import ipywidgets as widgets\n",
        "import uuid\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "from ipywidgets import interact, fixed\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functioni di utilità**\n",
        "Tramite la cella sottostante è possibile definire alcune funzioni di utilità usate durante l'esercitazione:\n",
        "- **create_mp4_video_from_frames** crea un video e lo salva in formato MP4 partendo da una lista di *frame*;\n",
        "- **draw_connected_components** visualizza le componenti connesse presenti su un'immagine binaria;\n",
        "- **draw_detected_objects** visualizza le *bounding-box* degli oggetti individuati su un singolo frame;\n",
        "- **draw_tracked_objects** visualizza gli oggetti tracciati su un singolo frame;\n",
        "- **compute_iou** calcola la *intersection over union* tra due *bounding-box*."
      ],
      "metadata": {
        "id": "ACCLZKLdI7iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mp4_video_from_frames(frames,fps):\n",
        "  temp_video_path='tempfile.mp4'\n",
        "  compressed_path='{}.mp4'.format(str(uuid.uuid4()))\n",
        "\n",
        "  size=(frames[0].shape[1],frames[0].shape[0])\n",
        "  out = cv2.VideoWriter(temp_video_path,cv2.VideoWriter_fourcc(*'mp4v'), fps, size)\n",
        "\n",
        "  for i in range(len(frames)):\n",
        "      out.write(frames[i][...,::-1].copy())\n",
        "  out.release()\n",
        "\n",
        "  os.system(f\"ffmpeg -i {temp_video_path} -vcodec libx264 {compressed_path}\")\n",
        "\n",
        "  os.remove(temp_video_path)\n",
        "\n",
        "  return compressed_path\n",
        "\n",
        "def draw_connected_components(labels):\n",
        "  label_hue = np.uint8(179*labels/np.max(labels))\n",
        "  blank_ch = 255*np.ones_like(label_hue)\n",
        "  labeled_img = cv2.merge([label_hue, blank_ch, blank_ch])\n",
        "\n",
        "  labeled_img = cv2.cvtColor(labeled_img, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "  labeled_img[label_hue==0] = 0\n",
        "\n",
        "  return labeled_img\n",
        "\n",
        "def draw_detected_objects(image,detected_bbs,detected_centroids,contours=None,detected_colors=(0,0,255)):\n",
        "  if contours is not None:\n",
        "    image_with_detected_objects=cv2.polylines(image.copy(),contours,True,(0,255,0),1)\n",
        "  else:\n",
        "    image_with_detected_objects=image.copy()\n",
        "\n",
        "  for i in range(len(detected_bbs)):\n",
        "    if type(detected_colors) is list:\n",
        "      color=detected_colors[i]\n",
        "    else:\n",
        "      color=detected_colors\n",
        "\n",
        "    image_with_detected_objects=cv2.rectangle(image_with_detected_objects,detected_bbs[i][0],detected_bbs[i][1],color,1)\n",
        "    image_with_detected_objects=cv2.circle(image_with_detected_objects,detected_centroids[i],3,color,-1)\n",
        "\n",
        "  return image_with_detected_objects\n",
        "\n",
        "def draw_tracked_objects(image,tracked_bbs,tracks,contours=None,tracked_colors=(0,0,255)):\n",
        "  if contours is not None:\n",
        "    image_with_tracked_objects=cv2.polylines(image.copy(),contours,True,(0,255,0),1)\n",
        "  else:\n",
        "    image_with_tracked_objects=image.copy()\n",
        "\n",
        "  for i in range(len(tracked_bbs)):\n",
        "    if type(tracked_colors) is list:\n",
        "      color=tracked_colors[i]\n",
        "    else:\n",
        "      color=tracked_colors\n",
        "\n",
        "    image_with_tracked_objects=cv2.rectangle(image_with_tracked_objects,tracked_bbs[i][0],tracked_bbs[i][1],color,1)\n",
        "    for j in range(len(tracks[i])-1):\n",
        "      image_with_tracked_objects=cv2.line(image_with_tracked_objects,tracks[i][j],tracks[i][j+1],color,1)\n",
        "\n",
        "  return image_with_tracked_objects\n",
        "\n",
        "def compute_iou(bb1, bb2):\n",
        "  bb1_x1=bb1[0][0]\n",
        "  bb1_y1=bb1[0][1]\n",
        "  bb1_x2=bb1[1][0]\n",
        "  bb1_y2=bb1[1][1]\n",
        "\n",
        "  bb2_x1=bb2[0][0]\n",
        "  bb2_y1=bb2[0][1]\n",
        "  bb2_x2=bb2[1][0]\n",
        "  bb2_y2=bb2[1][1]\n",
        "\n",
        "  x_left = max(bb1_x1, bb2_x1)\n",
        "  y_top = max(bb1_y1, bb2_y1)\n",
        "  x_right = min(bb1_x2, bb2_x2)\n",
        "  y_bottom = min(bb1_y2, bb2_y2)\n",
        "\n",
        "  if x_right < x_left or y_bottom < y_top:\n",
        "    return 0.0\n",
        "\n",
        "  intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
        "\n",
        "  bb1_area = (bb1_x2 - bb1_x1) * (bb1_y2 - bb1_y1)\n",
        "  bb2_area = (bb2_x2 - bb2_x1) * (bb2_y2 - bb2_y1)\n",
        "\n",
        "  iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
        "  assert iou >= 0.0\n",
        "  assert iou <= 1.0\n",
        "  return iou"
      ],
      "metadata": {
        "id": "gbcVnQb_I42e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Single object tracking**\n",
        "L'obiettivo è quello di sviluppare un'applicazione che, utilizzando l'algoritmo *mean-shift*, è in grado di seguire, all'interno di una sequenza video, il movimento di un **singolo** oggetto selezionato **manualmente** dall'utente.\n",
        "\n",
        "L'algoritmo *mean-shift* è composto da **due** fasi:\n",
        "1. calcolo delle **feature colore** dell'oggetto selezionato sul primo frame;\n",
        "\n",
        "<img src=https://biolab.csr.unibo.it/vr/esercitazioni/NotebookImages/EsObjectTracking\\MeanShiftFase1.png width=\"400\">\n",
        "\n",
        "2. per ogni successivo frame, ricerca del **candidato** più **simile** all'interno della **regione di interesse**.\n",
        "\n",
        "<img src=https://biolab.csr.unibo.it/vr/esercitazioni/NotebookImages/EsObjectTracking\\MeanShiftFase2.png width=\"800\">"
      ],
      "metadata": {
        "id": "1J-Fg6DKsA_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Caricare un video tramite OpenCV**\n",
        "OpenCV mette a disposizione la classe **VideoCapture** che permette di gestire video:\n",
        "- caricati da file;\n",
        "- rappresentati da sequenze di immagini;\n",
        "- acquisiti direttamente da una camera.\n",
        "\n",
        "Per poter creare un'istanza della classe a partire da un file è sufficiente richiamare il costruttore passando come parametro il percorso del file."
      ],
      "metadata": {
        "id": "bQlKR1MxaTOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_capture = cv2.VideoCapture('/content/mouthwash.avi')"
      ],
      "metadata": {
        "id": "_F1tYS46diBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il metodo **get(...)** della classe **VideoCapture** permette di accedere alle proprietà del video caricato."
      ],
      "metadata": {
        "id": "SWuwfGdZc-gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frame_count=int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "video_width=int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "video_height=int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "print('Numero totale di frame:',frame_count)\n",
        "print('Dimensione video:',video_width,'x',video_height)"
      ],
      "metadata": {
        "id": "32CXWS-Wcw9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fattore di resize**\n",
        "Nel caso che le dimensioni del video siano elevate, per ridurre i tempi di calcolo può essere utile ridimensionare i singoli frame prima di elaborarli. Per fare ciò, impostare nella cella sottostante il fattore di resize (*resize_factor*) desiderato (1=no resize)."
      ],
      "metadata": {
        "id": "Mlz9B6M1eFx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resize_factor=0.5\n",
        "\n",
        "resized_width=int(resize_factor*video_width)\n",
        "resized_height=int(resize_factor*video_height)\n",
        "\n",
        "print('Dimensione resize:',resized_width,'x',resized_height)"
      ],
      "metadata": {
        "id": "_mrs3FMVcm-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lettura di un singolo frame**\n",
        "Tramite il metodo **read(...)** è possibile ottenere il prossimo frame del video. Oltre al frame, il metodo restituisce un booleano che sarà impostato a *False* nel caso in cui non sia disponibile un nuovo frame (ad esempio perché la sequenza video è terminata). In tal caso verrà restituito un frame vuoto."
      ],
      "metadata": {
        "id": "E9l2g9GrcIpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ret,first_frame = video_capture.read()\n",
        "\n",
        "if resize_factor!=1:\n",
        "  first_frame=cv2.resize(first_frame,(resized_width,resized_height),cv2.INTER_LINEAR)\n",
        "\n",
        "print('Dimensione:',first_frame.shape)\n",
        "print('Formato:',first_frame.dtype)\n",
        "\n",
        "cv2_imshow(first_frame)"
      ],
      "metadata": {
        "id": "2IMq073tdoac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fase 1**\n",
        "L'obiettivo della fase 1 è calcolare le feature colore dell'oggetto che si vuole tracciare."
      ],
      "metadata": {
        "id": "-9WgI18c65hT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Selezione della regione di interesse**\n",
        "Tramite la cella sottostante è possibile selezionare la regione contenente l'oggetto da seguire.\n",
        "\n",
        "Per poter tracciare gli spostamenti della bottiglia si consiglia di selezionare la regione contenente l'etichetta."
      ],
      "metadata": {
        "id": "ZVNXBl0R68Jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_h_range=widgets.IntRangeSlider(description='Range Y:',min=0,max=first_frame.shape[0]-1,value=(0,first_frame.shape[0]-1), step=1,continuous_update=False)\n",
        "w_w_range=widgets.IntRangeSlider(description='Range X:',min=0,max=first_frame.shape[1]-1,value=(0,first_frame.shape[1]-1), step=1,continuous_update=False)\n",
        "\n",
        "@interact(frame=fixed(first_frame),h_range=w_h_range,w_range=w_w_range)\n",
        "def interactive_roi_selection(frame,h_range,w_range):\n",
        "  frame_with_roi=cv2.rectangle(frame.copy(),(w_range[0],h_range[0]),(w_range[1],h_range[1]),(0,0,255),1)\n",
        "  cv2_imshow(frame_with_roi)"
      ],
      "metadata": {
        "id": "YCRvx7soC4h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selezionata la regione di interesse (*RoI*), la rappresenteremo sotto forma di tupla: $(\\text{min}_X,\\text{min}_Y,W,H)$."
      ],
      "metadata": {
        "id": "3f4LCHv_9wAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_frame_roi_rect = (w_w_range.value[0],w_h_range.value[0],w_w_range.value[1]-w_w_range.value[0]+1,w_h_range.value[1]-w_h_range.value[0]+1)\n",
        "\n",
        "print('Regione di interesse:',first_frame_roi_rect)"
      ],
      "metadata": {
        "id": "RB-w89J08JOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizzando lo *slicing* degli **ndarray** e il rettangolo appena creato, ritagliare la *RoI* dal frame iniziale."
      ],
      "metadata": {
        "id": "9H_c44fYeyx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "roi=#...\n",
        "\n",
        "print('Dimensione:',roi.shape)\n",
        "\n",
        "cv2_imshow(roi)"
      ],
      "metadata": {
        "id": "CgmdSrJ8HLiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Calcolo delle feature colore**\n",
        "Come *feature* utilizzeremo l'**istogramma colore** calcolato sul canale H (tinta) nello spazio colore HSV. L'istogramma sarà calcolato sul canale H per ottenere feature colore più robuste rispetto a variazioni di luminosità e contrasto."
      ],
      "metadata": {
        "id": "xNDlKuLGgpci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Come prima cosa si dovrà convertire la regione di interesse dallo spazio colore BGR a quello HSV. Per farlo sarà sufficiente utilizzare la funzione **cvtColor(...)** di OpenCV."
      ],
      "metadata": {
        "id": "Cyv06hBeiaqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hsv_roi=#...\n",
        "\n",
        "print('Dimensione:',hsv_roi.shape)\n",
        "\n",
        "cv2_imshow(hsv_roi)"
      ],
      "metadata": {
        "id": "hiHI0L7O9KRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per aumentare la robustezza delle feature estratte, può essere utile calcolarle non sull'intera regione di interesse ma solo sui pixel che presentano i colori più caratteristici.\n",
        "\n",
        "La funzione **calcHist(...)** di OpenCV (che utilizzeremo per calcolare l'istogramma) può ricevere come parametro di input una maschera binaria che rappresenta i pixel su cui operare. Nella cella seguente si dovranno scegliere gli opportuni range di H, S e V per creare tale maschera avendo cura che solo la parte di interesse rimanga visibile mentre tutto ciò che non è influente venga mascherato."
      ],
      "metadata": {
        "id": "8XWg0qXfTYQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_h_range=widgets.IntRangeSlider(description='Range H:',min=0,max=255,value=(0,255), step=1,continuous_update=False)\n",
        "w_s_range=widgets.IntRangeSlider(description='Range S:',min=0,max=255,value=(0,255), step=1,continuous_update=False)\n",
        "w_v_range=widgets.IntRangeSlider(description='Range V:',min=0,max=255,value=(0,255), step=1,continuous_update=False)\n",
        "\n",
        "@interact(hsv_roi=fixed(hsv_roi),roi=fixed(roi),h_range=w_h_range,s_range=w_s_range,v_range=w_v_range)\n",
        "def interactive_masked_roi(hsv_roi,roi,h_range,s_range,v_range):\n",
        "  mask = cv2.inRange(hsv_roi, (h_range[0],s_range[0],v_range[0]), (h_range[1],s_range[1],v_range[1]))\n",
        "  cv2_imshow(cv2.bitwise_and(roi,roi, mask=mask))"
      ],
      "metadata": {
        "id": "kF_2C2WM8syM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creare la maschera binaria utilizzando la funzione **inRange(...)** passandogli in input i range individuati nella cella precedente."
      ],
      "metadata": {
        "id": "uStKxSH3XGe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h_min_value=w_h_range.value[0]\n",
        "h_max_value=w_h_range.value[1]\n",
        "s_min_value=w_s_range.value[0]\n",
        "s_max_value=w_s_range.value[1]\n",
        "v_min_value=w_v_range.value[0]\n",
        "v_max_value=w_v_range.value[1]\n",
        "\n",
        "roi_mask=#...\n",
        "\n",
        "cv2_imshow(roi_mask)"
      ],
      "metadata": {
        "id": "o0i3sg3UFTZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funzione **calcHist(...)** utilizzata per calcolare l'istogramma riceve in input diversi parametri tra cui:\n",
        "- il numero di *bin* dell'istogramma;\n",
        "- il range di valori da considerare sul canale selezionato.\n",
        "\n",
        "Utilizzare la cella sottostante per selezionare i valori opportuni per tali parametri."
      ],
      "metadata": {
        "id": "ULyELrxtXzoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_bin_count=widgets.IntSlider(min=10, max=256, step=1, value=256,continuous_update=False)\n",
        "w_h_range=widgets.IntRangeSlider(description='Range H:',min=0,max=255,value=(0,255), step=1,continuous_update=False)\n",
        "\n",
        "@interact(image=fixed(hsv_roi),channel=fixed(0),mask=fixed(roi_mask),bin_count=w_bin_count,h_range=w_h_range)\n",
        "def interactive_hist(image,channel,mask,bin_count,h_range):\n",
        "  hist = cv2.calcHist([image],[channel],mask,[bin_count],[h_range[0],h_range[1]])\n",
        "  plt.plot(hist)\n",
        "  plt.xlim([0,bin_count])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "i_NL02gGdrq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Richiamare la funzione **calcHist(...)** per calcolare l'istogramma sul canale H della regione di interesse (*hsv_roi*) utilizzando i parametri opportuni (*mask*, *bin_count* e *h_range*)."
      ],
      "metadata": {
        "id": "e0iv9tGkh1Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bin_count=w_bin_count.value\n",
        "h_range=[w_h_range.value[0],w_h_range.value[1]]\n",
        "\n",
        "roi_h_hist=#...\n",
        "\n",
        "plt.plot(roi_h_hist)\n",
        "plt.xlim([0,bin_count])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qZvP4rIRhN1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'altezza dell'istogramma calcolato dipende dal numero di pixel su cui è stato calcolato. Pertanto è opportuno normalizzarlo per renderlo indipendente dalla dimensione della regione di interesse.\n",
        "\n",
        "Il codice contenuto nella cella sottostante utilizza la funzione **normalize(...)** di OpenCV per normalizzare l'istogramma appena calcolato così che presenti un'altezza massima di 255 indipendentemente dal numero di pixel su cui è stato calcolato."
      ],
      "metadata": {
        "id": "7-TVfKR3i-hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv2.normalize(roi_h_hist,roi_h_hist,0,255,cv2.NORM_MINMAX)\n",
        "\n",
        "plt.plot(roi_h_hist)\n",
        "plt.xlim([0,bin_count])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M0_A1Qs6FNTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fase 2**\n",
        "L'obiettivo della fase 2 è ricercare in un frame la posizione del candidato più simile all'oggetto da tracciare selezionato nella fase 1 (in termini di feature colore).\n"
      ],
      "metadata": {
        "id": "zoeMW2KElgGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Calcolo della mappa di probabilità**\n",
        "Dato un  frame, per prima cosa si calcola la mappa di probabilità che indica la probabilità che ogni pixel appartenga a una regione il cui istogramma colore è simile a quello dell'oggetto *target*.\n",
        "\n",
        "La cella sottostante carica un nuovo frame."
      ],
      "metadata": {
        "id": "cfKpy6RDJ_QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_,frame = video_capture.read()\n",
        "\n",
        "if resize_factor!=1:\n",
        "  frame=cv2.resize(frame,(resized_width,resized_height),cv2.INTER_LINEAR)\n",
        "\n",
        "cv2_imshow(frame)"
      ],
      "metadata": {
        "id": "ThDY3VdhndQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertiamo il frame corrente nello spazio HSV tramite la funzione **cvtColor(...)**."
      ],
      "metadata": {
        "id": "dihA6MQDLmBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hsv_frame=#...\n",
        "\n",
        "cv2_imshow(hsv_frame)"
      ],
      "metadata": {
        "id": "gv6k1c-8nkQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Richiamando la funzione **calcBackProject(...)** di OpenCV si può calcolare la mappa di probabilità dati:\n",
        "- l'immagine di input (*hsv_frame*);\n",
        "- il canale su cui calcolarla (H=0);\n",
        "- l'istogramma dell'oggetto target (*roi_h_hist*);\n",
        "- il range di valori da considerare durante il calcolo dell'istogramma (*h_range*);\n",
        "- la scala con cui calcolare la mappa di probabilità (1)."
      ],
      "metadata": {
        "id": "n5se5lvSMBJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "back_proj=#...\n",
        "\n",
        "cv2_imshow(back_proj)"
      ],
      "metadata": {
        "id": "hNHpp1ttoEW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per ridurre la possibilità di incorrere in falsi positivi può essere utile considerare solo quei pixel che presentano colori simili a quelli che caratterizzano l'oggetto target.\n",
        "\n",
        "Per creare la maschera binaria dei pixel validi, è sufficiente utilizzare la funzione **inRange(...)** come abbiamo già fatto nella fase 1."
      ],
      "metadata": {
        "id": "2DSyC8aKOizG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frame_mask=#...\n",
        "\n",
        "cv2_imshow(frame_mask)"
      ],
      "metadata": {
        "id": "09uXSMD9pc0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizzando la funzione **bitwise_and(...)** è possibile annullare tutti i valori della mappa di probabilità che non corrispondono a pixel validi."
      ],
      "metadata": {
        "id": "qmWfPO86PUXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "back_proj=#...\n",
        "\n",
        "cv2_imshow(back_proj)"
      ],
      "metadata": {
        "id": "UCEB1YkWqIq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Individuazione del candidato più simile**\n",
        "Ora non resta che individuare l'oggetto più simile a quello target sulla base della mappa di probabilità appena calcolata.\n",
        "\n",
        "La funzione **meanShift(...)** di OpenCV restituisce la *bounding box* che contiene l'oggetto più simile a quello target, cercandolo vicino a dove si trovava nel frame precedente. I parametri di input sono:\n",
        "- la mappa di probabilità (*back_proj*);\n",
        "- la *bounding box* in cui si trovata l'oggetto target nel frame precedente;\n",
        "- il criterio di stop con cui terminare la ricerca (*term_crit*)."
      ],
      "metadata": {
        "id": "5h2_zS6OPs1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
        "\n",
        "_, roi_rect=#..."
      ],
      "metadata": {
        "id": "MLVp0k0prstc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La cella sottostante permette di visualizzare sulla mappa di probabilità la bounding box iniziale dell'oggetto target (in rosso) e la bounding box individuata sul frame corrente (in blu)."
      ],
      "metadata": {
        "id": "U-95c8IiT4Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "back_proj_with_roi=cv2.rectangle(cv2.cvtColor(back_proj.copy(), cv2.COLOR_GRAY2BGR),(first_frame_roi_rect[0],first_frame_roi_rect[1]),(first_frame_roi_rect[0]+first_frame_roi_rect[2],first_frame_roi_rect[1]+first_frame_roi_rect[3]),(0,0,255),1)\n",
        "back_proj_with_roi=cv2.rectangle(back_proj_with_roi,(roi_rect[0],roi_rect[1]),(roi_rect[0]+roi_rect[2],roi_rect[1]+roi_rect[3]),(255,0,0),1)\n",
        "cv2_imshow(back_proj_with_roi)"
      ],
      "metadata": {
        "id": "APEgYoy3T0QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tracciamento sull'intera sequenza video**\n",
        "Completare il codice sottostante per tracciare l'oggetto selezionato sull'intera sequenza video. In particolare:\n",
        "\n",
        "1. leggere un nuovo frame;\n",
        "2. ridimensionare il frame in base al *resize_factor*;\n",
        "3. convertire il frame nello spazio colore HSV;\n",
        "4. calcolare la mappa di probabilità;\n",
        "5. creare la maschera binaria dei pixel che presentano colori simili a quelli che caratterizzano l'oggetto target;\n",
        "6. mascherare la mappa di probabilità utilizzando la maschera creata al punto precedente;\n",
        "7. utilizzare l'algoritmo *mean-shift* per individuare la posizione corrente dell'oggetto.\n",
        "\n",
        "Il metodo **set(...)** della classe **VideoCapture** permette di impostare le proprietà di una istanza video. Nel nostro caso viene utilizzato per riportare all'inizio la posizione di lettura della sequenza video."
      ],
      "metadata": {
        "id": "-vKCEMJxU5lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracking_rgb_frames=[]\n",
        "tracking_back_projections=[]\n",
        "\n",
        "video_capture.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "\n",
        "roi_rect=first_frame_roi_rect\n",
        "while True:\n",
        "    #1\n",
        "    ret ,frame=#...\n",
        "\n",
        "    if ret:\n",
        "        #2\n",
        "        if resize_factor!=1:\n",
        "          frame=#...\n",
        "\n",
        "        #3\n",
        "        hsv_frame=#...\n",
        "\n",
        "        #4\n",
        "        back_proj=#...\n",
        "\n",
        "        #5\n",
        "        frame_mask=#...\n",
        "\n",
        "        #6\n",
        "        back_proj=#...\n",
        "\n",
        "        #7\n",
        "        _, roi_rect=#...\n",
        "\n",
        "        frame_with_roi = cv2.rectangle(frame, (roi_rect[0],roi_rect[1]), (roi_rect[0]+roi_rect[2],roi_rect[1]+roi_rect[3]), (0,0,255),1)\n",
        "        tracking_rgb_frames.append(cv2.cvtColor(frame_with_roi, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        back_proj_with_roi= cv2.rectangle(cv2.cvtColor(back_proj,cv2.COLOR_GRAY2RGB), (roi_rect[0],roi_rect[1]), (roi_rect[0]+roi_rect[2],roi_rect[1]+roi_rect[3]), (255,0,0),1)\n",
        "        tracking_back_projections.append(back_proj_with_roi)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "print('Numero di frame elaborati:',len(tracking_rgb_frames))"
      ],
      "metadata": {
        "id": "51pHZfBEF98o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il codice seguente crea un video in cui è riportato il risultato dell'applicazione implementata."
      ],
      "metadata": {
        "id": "wc6e34D1XreI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracking_combined=[]\n",
        "for i in range(len(tracking_rgb_frames)):\n",
        "  tracking_combined.append(np.concatenate((tracking_rgb_frames[i], tracking_back_projections[i]), axis=1))\n",
        "\n",
        "vide_file_name=create_mp4_video_from_frames(tracking_combined,30)\n",
        "\n",
        "mp4 = open(vide_file_name,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "HTML(\"\"\" <video controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
      ],
      "metadata": {
        "id": "kQcqAci6InM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Esercizio\n",
        "Testare l'applicazione appena implementata sul video \"nascar.mp4\"."
      ],
      "metadata": {
        "id": "gFW4VibhbzVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multiple object tracking**\n",
        "L'obiettivo è quello di sviluppare un'applicazione che, utilizzando l'algoritmo *Mixture Of Gaussian* per effettuare la sottrazione del background, è in grado di seguire oggetti **multipli** all'interno di una sequenza video.\n",
        "\n",
        "L'algoritmo è composto da **tre** fasi:\n",
        "\n",
        "1. **stima** del **background** calcolato su una porzione iniziale del flusso video;\n",
        "2. **individuazione** degli **oggetti** in movimento rispetto al background;\n",
        "3. **abbinamento** degli **oggetti** individuati con quelli individuati nel frame **precedente**."
      ],
      "metadata": {
        "id": "8FJpYkCzvWWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Caricamento del video**\n",
        "La cella sottostante crea un'istanza della classe **VideoCapture** caricando un file da disco."
      ],
      "metadata": {
        "id": "I6uYxOgBhpIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_capture = cv2.VideoCapture('/content/M6 Motorway Traffic.mp4')\n",
        "\n",
        "frame_count=int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "video_width=int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "video_height=int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "print('Numero totale di frame:',frame_count)\n",
        "print('Dimensione video:',video_width,'x',video_height)"
      ],
      "metadata": {
        "id": "v-MkHfGEyGcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fattore di resize**\n",
        "Nel caso che le dimensioni del video siano elevate, per ridurre i tempi di calcolo può essere utile ridimensionare i singoli frame prima di elaborarli. Per fare ciò, impostare nella cella sottostante il fattore di resize (*resize_factor*) desiderato (1=no resize)."
      ],
      "metadata": {
        "id": "iPzyhFP8iWV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resize_factor=0.5\n",
        "\n",
        "resized_width=int(resize_factor*video_width)\n",
        "resized_height=int(resize_factor*video_height)\n",
        "\n",
        "print('Dimensione resize:',resized_width,'x',resized_height)"
      ],
      "metadata": {
        "id": "XZBtWpAv1_Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lettura primo frame**\n",
        "Eseguendo la cella seguente verrà letto il primo frame della sequenza video caricata. Nel caso che *resize_factor* sia diverso da 1, il frame verrà ridimensionato in maniera opportuna."
      ],
      "metadata": {
        "id": "CQ3HXVO9iuQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_,first_frame = video_capture.read()\n",
        "\n",
        "if resize_factor!=1:\n",
        "  first_frame=cv2.resize(first_frame,(resized_width,resized_height),cv2.INTER_LINEAR)\n",
        "\n",
        "print('Dimensione:',first_frame.shape)\n",
        "print('Formato:',first_frame.dtype)\n",
        "\n",
        "cv2_imshow(first_frame)"
      ],
      "metadata": {
        "id": "RZsGA5Ezyk3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Regione di interesse**\n",
        "In questo tipo di applicazioni può essere utile definire la regione di interesse su cui l'algoritmo dovrà lavorare così da ridurre la presenza di falsi positivi in zone ininfluenti.\n",
        "\n",
        "La cella seguente definisce la regione di interesse come un insieme di poligoni e la visualizza sul frame corrente."
      ],
      "metadata": {
        "id": "6esULsRxkgfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "left_side_contour=np.array([[0,719],[0,510], [515,280],[625,280],[490,719] ])\n",
        "right_side_contour=np.array([[790,719],[655,280],[750,280], [1279,560],[1279,719] ])\n",
        "\n",
        "if resize_factor!=1:\n",
        "  for i in range(len(left_side_contour)):\n",
        "      left_side_contour[i]=left_side_contour[i]*resize_factor\n",
        "  for i in range(len(right_side_contour)):\n",
        "      right_side_contour[i]=right_side_contour[i]*resize_factor\n",
        "\n",
        "frame_with_roi=cv2.polylines(first_frame,[left_side_contour,right_side_contour],True,(0,255,0),1)\n",
        "\n",
        "cv2_imshow(frame_with_roi)"
      ],
      "metadata": {
        "id": "OvoJ57yw2jj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per poter utilizzare in maniera efficiente la regione di interesse, è bene rappresentarla con una maschera binaria."
      ],
      "metadata": {
        "id": "_hcsobAGlUrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "roi_mask=np.zeros((resized_height,resized_width),np.uint8)\n",
        "cv2.fillPoly(roi_mask, pts = [left_side_contour,right_side_contour], color =(255,255,255))\n",
        "\n",
        "cv2_imshow(roi_mask)"
      ],
      "metadata": {
        "id": "rxB9Iddf3E01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fase 1**\n",
        "L'obiettivo della fase 1 è la stima del background partendo dalla porzione iniziale del flusso video."
      ],
      "metadata": {
        "id": "4tyj1dzPlnas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La cella seguente calcola il numero di frame da utilizzare nella stima del background a partire da una percentuale di input."
      ],
      "metadata": {
        "id": "A-F5sJVTmKGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bg_est_frame_perc = 0.01\n",
        "\n",
        "train_frame_count=int(frame_count*bg_est_frame_perc)\n",
        "\n",
        "print('Numero di frame utilizzati per la stima del BG:',train_frame_count)"
      ],
      "metadata": {
        "id": "A3lJbNyQ3kht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creazione della *Mixture Of Gaussian***\n",
        "La funzione **createBackgroundSubtractorMOG2(...)** di OpenCV crea un'istanza della classe **BackgroundSubtractorMOG2** che implementa un algoritmo di sottrazione del background basato su *Mixture Of Gaussian*."
      ],
      "metadata": {
        "id": "69iCAt8q5Ngn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bg_subtractor = cv2.createBackgroundSubtractorMOG2()"
      ],
      "metadata": {
        "id": "yq80x_or5CrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stima background iniziale**\n",
        "Completare il codice sottostante per stimare il background iniziale. In particolare:\n",
        "\n",
        "1. leggere un nuovo frame;\n",
        "2. ridimensionare il frame in base al *resize_factor*;\n",
        "3. aggiornare il background tramite il metodo **apply(...)** della classe **BackgroundSubtractorMOG2**;\n",
        "4. memorizzare l'immagine del background attuale nella variabile *current_bg* tramite il metodo **getBackgroundImage(...)**.\n",
        "\n",
        "Il metodo **apply(...)**, presi in input un frame e il *learning rate*, sottrae il background dal frame di input restituendo la maschera di foreground dopodiché aggiorna il background. Visto che in questa fase vogliamo stimare il backgorund iniziale, non sarà necessario memorizzare la maschera di foreground restituita. Infine, se il *learning rate* non viene passato al metodo, l'algoritmo calcola internamente il *learning rate* più opportuno.\n",
        "\n",
        "Attenzione, se si vuole provare a stimare il background al variare del numero di frame e del *learning rate* utilizzati, è necessario ogni volta re-inizializzare la variabile *bg_subtractor*."
      ],
      "metadata": {
        "id": "9xgaMZ7j6jnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_capture.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "\n",
        "estimated_bg=[]\n",
        "k=0\n",
        "while k<train_frame_count:\n",
        "  #1\n",
        "  ret, frame=#...\n",
        "  if ret==False:\n",
        "    break\n",
        "\n",
        "  #2\n",
        "  if resize_factor!=1:\n",
        "    frame=#...\n",
        "\n",
        "  #3\n",
        "  #...\n",
        "\n",
        "  #4\n",
        "  current_bg=#...\n",
        "\n",
        "  estimated_bg.append(cv2.cvtColor(current_bg, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "  k+=1"
      ],
      "metadata": {
        "id": "4IZGohPx5Onr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il codice seguente crea un video in cui è mostrata l'evoluzione del background stimato dalla *Mixture Of Gaussian*."
      ],
      "metadata": {
        "id": "j31IHJpNBRmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vide_file_name=create_mp4_video_from_frames(estimated_bg,120)\n",
        "\n",
        "mp4 = open(vide_file_name,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "HTML(\"\"\" <video controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
      ],
      "metadata": {
        "id": "vCNuzrhV-pum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fase 2**\n",
        "L'obiettivo della fase 2 è, dato un frame, individuare la posizione degli oggetti in movimento."
      ],
      "metadata": {
        "id": "tanstthhCDEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La cella sottostante carica un nuovo frame."
      ],
      "metadata": {
        "id": "7e5urUp-fnB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ ,frame = video_capture.read()\n",
        "\n",
        "if resize_factor!=1:\n",
        "  frame=cv2.resize(frame,(resized_width,resized_height),cv2.INTER_LINEAR)\n",
        "\n",
        "print('Dimensione:',first_frame.shape)\n",
        "print('Formato:',first_frame.dtype)\n",
        "\n",
        "cv2_imshow(frame)"
      ],
      "metadata": {
        "id": "3l_09Yt7gJDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Segmentazione del foreground**\n",
        "Utilizzare il metodo **apply(...)** per sottrarre il background dal frame corrente e ottenere la maschera di *foreground*.\n",
        "\n",
        "Si noti che, se il *learning_rate* è diverso da zero, il background verrà mantenuto aggiornato."
      ],
      "metadata": {
        "id": "lN-3JiELf3TK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fg=#...\n",
        "\n",
        "print('Dimensione:',fg.shape)\n",
        "print('Formato:',fg.dtype)\n",
        "\n",
        "cv2_imshow(fg)"
      ],
      "metadata": {
        "id": "fCD6px4dgtOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Post-processing del foreground**\n",
        "La maschera di foreground così ottenuta necessità di alcune operazioni di post-processing per migliorarne la qualità e rendere più robusta l'individuazione degli oggetti in movimento.  "
      ],
      "metadata": {
        "id": "uhlaFBk5T2iX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I pixel dell'immagine restituita dal metodo **apply(...)** della classe **BackgroundSubtractorMOG2** presentano tre possibili valori di intensità corrispondenti a tre diverse classi:\n",
        "\n",
        "- background (0);\n",
        "- shadows (127);\n",
        "- foreground (255).\n",
        "\n",
        "È possibile ottenere una maschera binaria eliminando le ombre (*shadows*) tramite la funzione **threshold(...)** con soglia di binarizzazione maggiore di 127."
      ],
      "metadata": {
        "id": "11y0yjrYR8Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bin_thr=128\n",
        "\n",
        "_,fg=#...\n",
        "\n",
        "print('Dimensione:',fg.shape)\n",
        "print('Formato:',fg.dtype)\n",
        "\n",
        "cv2_imshow(fg)"
      ],
      "metadata": {
        "id": "KHRgONIoixfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solo gli oggetti in movimento all'interno della regione di interesse sono significativi ai fini dell'applicazione.\n",
        "\n",
        "Richiamare la funzione **bitwise_and(...)** di OpenCV per eseguire l'AND logico tra i rispettivi pixel delle maschere binarie di *foregorund* e della regione di interesse."
      ],
      "metadata": {
        "id": "klzPW5IsOp4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fg=#...\n",
        "\n",
        "print('Dimensione:',fg.shape)\n",
        "print('Formato:',fg.dtype)\n",
        "\n",
        "cv2_imshow(fg)"
      ],
      "metadata": {
        "id": "z3aEfw4qjCrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per eliminare piccole zone di rumore e rendere più omogenee le aree di foreground possono essere utilizzati gli operatori morfologici.\n",
        "\n",
        "Il codice sottostante crea un filtro (o *kernel*) di forma circolare con dimensione *morph_kernel_size*$\\times$*morph_kernel_size*."
      ],
      "metadata": {
        "id": "glAQnEhzf5z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "morph_kernel_size=5\n",
        "\n",
        "half_morph_kernel_size=int(morph_kernel_size/2)\n",
        "\n",
        "morph_kernel=np.zeros((morph_kernel_size, morph_kernel_size), dtype = \"uint8\")\n",
        "cv2.circle(morph_kernel, (half_morph_kernel_size, half_morph_kernel_size), half_morph_kernel_size, 255, -1)\n",
        "\n",
        "print('Dimensione:',morph_kernel.shape)\n",
        "print('Formato:',morph_kernel.dtype)\n",
        "\n",
        "print(morph_kernel)"
      ],
      "metadata": {
        "id": "NJdhMd_cjRLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chiudere i piccoli buchi presenti negli oggetti applicando l'operatore morfologico di **chiusura** alla maschera di foreground utilizzando il kernel creato in precedenza.\n",
        "\n",
        "Per farlo, richiamare la funzione **morphologyEx(...)** di OpenCV con il parametro *op* uguale a cv2.MORPH_CLOSE."
      ],
      "metadata": {
        "id": "YlKImhR-g_nJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fg=#...\n",
        "\n",
        "print('Dimensione:',fg.shape)\n",
        "print('Formato:',fg.dtype)\n",
        "\n",
        "cv2_imshow(fg)"
      ],
      "metadata": {
        "id": "jvq91VifjoT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rimuovere piccoli oggetti (rumore) utilizzando l'operatore morfologico di **apertura** usando lo stesso kernel utilizzato per l'operazione precedente.\n",
        "\n",
        "La funzione **morphologyEx(...)** può essere utilizzata per eseguire la chiusura impostando il parametro *op* uguale a cv2.MORPH_OPEN."
      ],
      "metadata": {
        "id": "3hzTuhabinLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fg=#...\n",
        "\n",
        "print('Dimensione:',fg.shape)\n",
        "print('Formato:',fg.dtype)\n",
        "\n",
        "cv2_imshow(fg)"
      ],
      "metadata": {
        "id": "CqVbf20nhXxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Infine applicare un operatore di **dilatazione** per chiudere (o ridurre) i buchi rimasti oltre che per ingrandire leggermente gli oggetti di foreground.\n",
        "\n",
        "La funzione **morphologyEx(...)** può essere utilizzata per eseguire la dilatazione impostando il parametro *op* uguale a cv2.MORPH_DILATE."
      ],
      "metadata": {
        "id": "pkO8VxKBjX6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fg=#...\n",
        "\n",
        "print('Dimensione:',fg.shape)\n",
        "print('Formato:',fg.dtype)\n",
        "\n",
        "cv2_imshow(fg)"
      ],
      "metadata": {
        "id": "5TxBH9o7hX7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Individuazione dei possibili candidati**\n",
        "Per individuare tutti i possibili candidati presenti nella maschera di foreground ottenuta dopo la fase di post-processing si usi l'algoritmo delle componenti connesse.\n",
        "\n",
        "Richiamare la funzione di OpenCV **connectedComponentsWithStats(...)** passandogli in input la maschera di foregound. La funzione restituirà:\n",
        "\n",
        "- il numero di oggetti trovati (*label_count*);\n",
        "- un'immagine in cui ad ogni pixel è assegnato l'indice dell'oggetto a cui appartiene (*labels*);\n",
        "- un array contenente la dimensione (in pixel) e la *bounding box* di ogni oggetto (*stats*);\n",
        "- un array con le coordinate dei centroidi di ogni oggetto (*centroids*).\n",
        "\n",
        "La funzione **draw_connected_components(...)** permette di visualizzare ogni candidato con un colore diverso."
      ],
      "metadata": {
        "id": "bwEmmPPmk7XB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_count,labels,stats,centroids=#...\n",
        "\n",
        "print(label_count)\n",
        "\n",
        "cv2_imshow(draw_connected_components(labels))"
      ],
      "metadata": {
        "id": "zhI8jXYgj-92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non sempre i candidati individuati in questa fase rappresentano degli oggetti in movimento; in alcuni casi potrebbero essere frutto di rumore che non si è riusciti a rimuovere nella fase di post-processing.\n",
        "\n",
        "I candidati validi possono essere selezionati sulla base di specifiche caratteristiche quali: area, perimetro, forma, feature colore, ecc..\n",
        "\n",
        "In questa applicazione i candidati validi sono selezionati sulla base della loro area utilizzando la cella sottostante."
      ],
      "metadata": {
        "id": "feFwMhM5ntg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "minimum_valid_area = 100\n",
        "maximum_valid_area = 15000\n",
        "\n",
        "detected_bbs=[]\n",
        "detected_centroids=[]\n",
        "for i in range(label_count):\n",
        "    if stats[i,cv2.CC_STAT_AREA]>=minimum_valid_area and stats[i,cv2.CC_STAT_AREA]<=maximum_valid_area:\n",
        "        detected_bbs.append([(stats[i,cv2.CC_STAT_LEFT],stats[i,cv2.CC_STAT_TOP]),(stats[i,cv2.CC_STAT_LEFT]+stats[i,cv2.CC_STAT_WIDTH],stats[i,cv2.CC_STAT_TOP]+stats[i,cv2.CC_STAT_HEIGHT])])\n",
        "        detected_centroids.append(centroids[i].astype(int))\n",
        "\n",
        "frame_with_objects=draw_detected_objects(frame,detected_bbs,detected_centroids,[left_side_contour,right_side_contour])\n",
        "\n",
        "cv2_imshow(frame_with_objects)"
      ],
      "metadata": {
        "id": "hcdoX-jal17v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Funzione detect_objects**\n",
        "Completare la funzione sottostante che esegue tutti i passi della fase 2 dati in input:\n",
        "- il frame corrente (*frame*);\n",
        "- la maschera binaria della regione di interesse (*roi_mask*);\n",
        "- l'istanza della classe **BackgroundSubtractorMOG2** con cui è stato stimato il backgorund iniziale nella fase 1 (*bg_subtractor*);\n",
        "- la soglia di binarizzazione della maschera di foreground (*bin_thr*);\n",
        "- la dimensione minima di un oggetto valido (*minimum_valid_area*);\n",
        "- la dimensione massima di un oggetto valido (*maximum_valid_area*);\n",
        "- il kernel da utilizzare con gli operatori morfologici (*morph_kernel*).  \n",
        "\n",
        "Seguendo le operazioni svolte precedentemente si dovrà:\n",
        "1. segmentare il foreground tramite il metodo **apply(...)**;\n",
        "2. segmentare la maschera di foreground per rimuovere le ombre tramite la funzione **threshold(...)**;\n",
        "3. intersecare le maschere binarie di foreground e della regione di interesse utilizzando l'operatore logico AND tramite la funzione **bitwise_and(...)**;\n",
        "4. applicare gli operatori morfologici di chiudura, apertura e dilatazione tramite la funzione **morphologyEx(...)**;\n",
        "5. richiamare la funzione **connectedComponentsWithStats(...)** per l'individuazione dei candidati tramite l'etichettatura delle componenti connesse."
      ],
      "metadata": {
        "id": "TpeoZpFpq3hY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_objects(frame,roi_mask,bg_subtractor,bin_thr,minimum_valid_area,maximum_valid_area,morph_kernel=None):\n",
        "  #1\n",
        "  fg=#...\n",
        "\n",
        "  #2\n",
        "  _,fg=#...\n",
        "\n",
        "  #3\n",
        "  fg=#...\n",
        "\n",
        "  #4\n",
        "  if morph_kernel is not None:\n",
        "    fg=#...\n",
        "    fg=#...\n",
        "    fg=#...\n",
        "\n",
        "  #5\n",
        "  label_count,labels,stats,centroids=#...\n",
        "\n",
        "  detected_bbs=[]\n",
        "  detected_centroids=[]\n",
        "  for i in range(label_count):\n",
        "    if stats[i,cv2.CC_STAT_AREA]>=minimum_valid_area and stats[i,cv2.CC_STAT_AREA]<=maximum_valid_area:\n",
        "      detected_bbs.append([(stats[i,cv2.CC_STAT_LEFT],stats[i,cv2.CC_STAT_TOP]),(stats[i,cv2.CC_STAT_LEFT]+stats[i,cv2.CC_STAT_WIDTH],stats[i,cv2.CC_STAT_TOP]+stats[i,cv2.CC_STAT_HEIGHT])])\n",
        "      detected_centroids.append(centroids[i].astype(int))\n",
        "\n",
        "  return detected_bbs,detected_centroids,fg"
      ],
      "metadata": {
        "id": "D-RfY3gAn9Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Individuazione sulla sequenza video**\n",
        "Completare il codice sottostante per individuare la posizione degli oggetti in movimento all'interno della sequenza video. In particolare:\n",
        "\n",
        "1. leggere un nuovo frame;\n",
        "2. ridimensionare il frame in base al *resize_factor*;\n",
        "3. utilizzare la funzione **detect_object(...)** appena implementata per individuare la posizione degli oggetti in movimento all'interno del frame corrente."
      ],
      "metadata": {
        "id": "jS9ZgnRXvPVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "detection_frame_count=500\n",
        "\n",
        "video_capture.set(cv2.CAP_PROP_POS_FRAMES, train_frame_count)\n",
        "\n",
        "video_detected_objects=[]\n",
        "fg_detected_objects=[]\n",
        "frame_count=0\n",
        "while frame_count<detection_frame_count:\n",
        "  #1\n",
        "  ret ,frame=#...\n",
        "\n",
        "  if ret == False:\n",
        "    break\n",
        "\n",
        "  frame_count+=1\n",
        "\n",
        "  #2\n",
        "  if resize_factor!=1:\n",
        "    frame=#...\n",
        "\n",
        "  #3\n",
        "  detected_bbs,detected_centroids,fg=#...\n",
        "\n",
        "  frame_with_objects=draw_detected_objects(frame,detected_bbs,detected_centroids,[left_side_contour,right_side_contour])\n",
        "  fg_with_objects=draw_detected_objects(cv2.cvtColor(fg, cv2.COLOR_GRAY2BGR),detected_bbs,detected_centroids,[left_side_contour,right_side_contour])\n",
        "\n",
        "  video_detected_objects.append(frame_with_objects)\n",
        "  fg_detected_objects.append(fg_with_objects)"
      ],
      "metadata": {
        "id": "e9LU3iMboR0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il codice seguente crea un video in cui è riportato il risultato del codice appena eseguito."
      ],
      "metadata": {
        "id": "ejffYs0CwQI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_detection_both=[]\n",
        "for i in range(len(video_detected_objects)):\n",
        "  video_detection_both.append(cv2.cvtColor(np.concatenate((video_detected_objects[i], fg_detected_objects[i]), axis=1),cv2.COLOR_BGR2RGB))\n",
        "\n",
        "vide_file_name=create_mp4_video_from_frames(video_detection_both,30)\n",
        "\n",
        "mp4 = open(vide_file_name,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "HTML(\"\"\" <video controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
      ],
      "metadata": {
        "id": "8K1qKTuStRoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fase 3**\n",
        "L'obiettivo della fase 3 è, dati due frame consecutivi su cui sono stati individuati gli oggetti in movimento (fase 2), abbinare gli oggetti individuati nel frame corrente con quelli individuati nel frame precedente."
      ],
      "metadata": {
        "id": "nfxCCcF3w12M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il codice contenuto nella cella sottostante:\n",
        "1. legge due frame consecutivi dalla sequenza video;\n",
        "2. ridimensiona i frame in base al *resize_factor*;\n",
        "3. individua gli oggetti in movimento su entrambi i frame utilizzando la funzione **detect_objects(...)** implementata nella fase 2."
      ],
      "metadata": {
        "id": "BfQ3liT-x29M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_capture.set(cv2.CAP_PROP_POS_FRAMES, train_frame_count)\n",
        "\n",
        "#1\n",
        "_,frame_a = video_capture.read()\n",
        "_,frame_b = video_capture.read()\n",
        "\n",
        "#2\n",
        "if resize_factor!=1:\n",
        "  frame_a=cv2.resize(frame_a,(resized_width,resized_height),cv2.INTER_LINEAR)\n",
        "  frame_b=cv2.resize(frame_b,(resized_width,resized_height),cv2.INTER_LINEAR)\n",
        "\n",
        "#3\n",
        "detected_bbs_a,detected_centroids_a,_=detect_objects(frame_a,roi_mask,bg_subtractor,bin_thr,minimum_valid_area,maximum_valid_area,morph_kernel)\n",
        "detected_bbs_b,detected_centroids_b,_=detect_objects(frame_b,roi_mask,bg_subtractor,bin_thr,minimum_valid_area,maximum_valid_area,morph_kernel)\n",
        "\n",
        "frame_a_with_objects=draw_detected_objects(frame_a,detected_bbs_a,detected_centroids_a,[left_side_contour,right_side_contour],(255,0,0))\n",
        "frame_b_with_objects=draw_detected_objects(frame_b,detected_bbs_b,detected_centroids_b,[left_side_contour,right_side_contour],(255,0,0))\n",
        "\n",
        "cv2_imshow(np.concatenate((frame_a_with_objects, frame_b_with_objects), axis=1))"
      ],
      "metadata": {
        "id": "JnpXVCdvvVUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Abbinamento oggetti tramite Intersection over Union**\n",
        "L'obiettivo del codice sottostante è quello di selezionare i soli oggetti del frame corrente (*frame_b*) che erano già presenti nel frame precedente (*frame_a*).\n",
        "\n",
        "Per farlo, completare la funzione sottostante che implementa il seguente algoritmo:\n",
        "<BR></BR>\n",
        "Per ogni *bounding box* contenente un oggetto del frame precedente (*detected_bbs_a*), calcolare l'*Intersection over Union* con tutte le *bounding box* degli oggetti del frame corrente (*detected_bbs_b*) e selezionare l'oggetto del frame corrente che presenta l'*Intersection over Union* massima solamente se questa risulta maggiore della soglia *min_iou*.\n",
        "<BR></BR>\n",
        "In particolare si implementino le seguenti operazioni:\n",
        "1. calcolare l'*Intersection over Union* tra l'*i*-esima *bounding box* del frame precedente e la *j*-esima *bounding box* del frame corrente richiamando la funzione **compute_iou(...)** definita all'inizio dell'esercitazione;\n",
        "2. se l'*Intersection over Union* calcolata al punto precedente è maggiore della massima trovata fino ad ora (*max_iou*), aggiornare *max_iou* e *max_iou_idx*;\n",
        "3. se l'*Intersection over Union* massima è maggiore della soglia *min_iou*, aggiungere la *bounding box* del frame corrente in posizione *max_iou_idx* alla lista delle *bounding box* abbinate *tracked_bbs*."
      ],
      "metadata": {
        "id": "Sod6qnbQytTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def track_objects(prev_tracked_bbs,prev_tracks,curr_detected_bbs,curr_detected_centroids,min_iou):\n",
        "  tracked_bbs=[]\n",
        "  tracks=[]\n",
        "  curr_tracked_objects=set()\n",
        "  for i in range(len(prev_tracked_bbs)):\n",
        "    max_iou=0\n",
        "    max_iou_idx=None\n",
        "    for j in range(len(curr_detected_bbs)):\n",
        "      if j not in curr_tracked_objects:\n",
        "        #1\n",
        "        iou=#...\n",
        "        #2\n",
        "        if iou>max_iou:\n",
        "          max_iou=#...\n",
        "          max_iou_idx=#...\n",
        "\n",
        "    if max_iou_idx is not None and max_iou>min_iou:\n",
        "      #3\n",
        "      tracked_bbs.append(#...)\n",
        "      prev_track=prev_tracks[i]\n",
        "      prev_track.append(curr_detected_centroids[max_iou_idx])\n",
        "      tracks.append(prev_track)\n",
        "      curr_tracked_objects.add(max_iou_idx)\n",
        "\n",
        "  return tracked_bbs,tracks,curr_tracked_objects"
      ],
      "metadata": {
        "id": "W9rs_c8oJO3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La cella seguente esegue la funzione **track_object(...)** appena implementata e ne visualizza il risultato."
      ],
      "metadata": {
        "id": "7wTDyfhwGVJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_iou = 0.3\n",
        "\n",
        "cc=[list([c]) for c in detected_centroids_a]\n",
        "\n",
        "tracked_bbs,tracks,curr_tracked_objects=track_objects(detected_bbs_a,cc,detected_bbs_b,detected_centroids_b,min_iou)\n",
        "\n",
        "frame_with_tracked_objects=draw_tracked_objects(frame_b,tracked_bbs,tracks,[left_side_contour,right_side_contour],(0,0,255))\n",
        "\n",
        "cv2_imshow(np.concatenate((frame_a_with_objects,frame_b_with_objects, frame_with_tracked_objects), axis=1))"
      ],
      "metadata": {
        "id": "f6GVv6qYjg7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Inserimento nuovi oggetti**\n",
        "Può succedere che nel frame corrente siano presenti oggetti appena entrati in scena (oppure non rilevati nel frame precedente) che vengono scartati dalla funzione **track_objects(...)** perchè non hanno un oggetto associato nel frame precedente.\n",
        "\n",
        "Perciò è opportuno aggiungere agli output ottenuti tutti questi nuovi oggetti così da porterne tracciare il movimento nei frame successivi.\n",
        "\n",
        "Il codice sottostante aggiunge i nuovi oggetti all'elenco degli oggetti tracciati e visualizza a video il risultato."
      ],
      "metadata": {
        "id": "9TpOGcj5G1hL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracked_colors=[(0,0,255)] *len(tracked_bbs)\n",
        "tracked_colors=tracked_colors+[(255,0,0) for i in range(len(detected_bbs_b)) if i not in curr_tracked_objects]\n",
        "\n",
        "tracked_bbs=tracked_bbs+[detected_bbs_b[i] for i in range(len(detected_bbs_b)) if i not in curr_tracked_objects]\n",
        "tracks=tracks+[[detected_centroids_b[i]] for i in range(len(detected_bbs_b)) if i not in curr_tracked_objects]\n",
        "\n",
        "frame_with_tracked_and_new_objects=draw_tracked_objects(frame_b,tracked_bbs,tracks,[left_side_contour,right_side_contour],tracked_colors=tracked_colors)\n",
        "\n",
        "cv2_imshow(frame_with_tracked_and_new_objects)"
      ],
      "metadata": {
        "id": "ofG_zl18H5A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tracciamento sulla sequenza video**\n",
        "Completare il codice sottostante per tracciare gli oggetti in movimento all'interno della sequenza video. In particolare:\n",
        "\n",
        "1. leggere un nuovo frame;\n",
        "2. ridimensionare il frame in base al *resize_factor*;\n",
        "3. utilizzare la funzione **detect_object(...)** per individuare la posizione degli oggetti in movimento all'interno del frame corrente;\n",
        "4. utilizzare la funzione **track_objects(...)** per abbinare gli oggetti in movimento individuati nel frame corrente con gli oggetti tracciati dai frame precedenti."
      ],
      "metadata": {
        "id": "j2frINcVNoBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracking_frame_count=500\n",
        "min_iou=0.3\n",
        "\n",
        "video_capture.set(cv2.CAP_PROP_POS_FRAMES, train_frame_count)\n",
        "\n",
        "video_tracked_objects=[]\n",
        "fg_tracked_objects=[]\n",
        "prev_tracked_bbs=[]\n",
        "prev_tracks=[]\n",
        "frame_count=0\n",
        "while frame_count<tracking_frame_count:\n",
        "  #1\n",
        "  ret ,frame=#...\n",
        "\n",
        "  if ret == False:\n",
        "    break\n",
        "\n",
        "  frame_count+=1\n",
        "\n",
        "  #2\n",
        "  if resize_factor!=1:\n",
        "    frame=#...\n",
        "\n",
        "  #3\n",
        "  detected_bbs,detected_centroids,fg=#...\n",
        "\n",
        "  #4\n",
        "  tracked_bbs,tracks,curr_tracked_objects=#...\n",
        "\n",
        "  prev_tracked_bbs=tracked_bbs+[detected_bbs[i] for i in range(len(detected_bbs)) if i not in curr_tracked_objects]\n",
        "  prev_tracks=tracks+[[detected_centroids[i]] for i in range(len(detected_bbs)) if i not in curr_tracked_objects]\n",
        "\n",
        "  frame_with_tracked_objects=draw_tracked_objects(frame,tracked_bbs,tracks,[left_side_contour,right_side_contour])\n",
        "  fg_with_tracked_objects=draw_tracked_objects(cv2.cvtColor(fg, cv2.COLOR_GRAY2BGR),tracked_bbs,tracks,[left_side_contour,right_side_contour])\n",
        "\n",
        "  video_tracked_objects.append(frame_with_tracked_objects)\n",
        "  fg_tracked_objects.append(fg_with_tracked_objects)"
      ],
      "metadata": {
        "id": "6p26N-qaLFoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il codice seguente crea un video in cui è riportato il risultato del codice appena eseguito."
      ],
      "metadata": {
        "id": "o7mKWfKrfo0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_track_both=[]\n",
        "for i in range(len(video_tracked_objects)):\n",
        "  video_track_both.append(cv2.cvtColor(np.concatenate((video_tracked_objects[i], fg_tracked_objects[i]), axis=1),cv2.COLOR_BGR2RGB))\n",
        "\n",
        "vide_file_name=create_mp4_video_from_frames(video_track_both,30)\n",
        "\n",
        "mp4 = open(vide_file_name,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "HTML(\"\"\" <video controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
      ],
      "metadata": {
        "id": "rjLJ0bxZM9y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Esercizio\n",
        "Testare l'applicazione appena implementata sul video \"TownCentreXVID.avi\"."
      ],
      "metadata": {
        "id": "OZCXBRD9f7ER"
      }
    }
  ]
}